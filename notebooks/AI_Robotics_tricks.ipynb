{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goncalog/ai-robotics/blob/main/notebooks/AI_Robotics_tricks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "Jz-I63-7YgRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "debug = False\n",
        "\n",
        "num_timesteps = 30_000_000\n",
        "num_evals = 8\n",
        "learning_rate = 3e-4\n",
        "# learning_rate = 3e-5\n",
        "# learning_rate = 3e-6\n",
        "# learning_rate = 3e-7\n",
        "\n",
        "model_path = '/tmp/mjx_brax_policy'\n",
        "upload_model = False\n",
        "\n",
        "num_rollouts = 1\n",
        "score_threshold = 0"
      ],
      "metadata": {
        "id": "-Xt8DyfJYrbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvyGCsgSCxHQ"
      },
      "source": [
        "# Install MuJoCo, MJX, and Brax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xqo7pyX-n72M"
      },
      "outputs": [],
      "source": [
        "!pip install mujoco\n",
        "!pip install mujoco_mjx\n",
        "!pip install brax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbZxYDxzoz5R",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Check if MuJoCo installation was successful\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "import distutils.util\n",
        "import os\n",
        "import subprocess\n",
        "if subprocess.run('nvidia-smi').returncode:\n",
        "  raise RuntimeError(\n",
        "      'Cannot communicate with GPU. '\n",
        "      'Make sure you are using a GPU Colab runtime. '\n",
        "      'Go to the Runtime menu and select Choose runtime type.')\n",
        "\n",
        "# Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
        "# This is usually installed as part of an Nvidia driver package, but the Colab\n",
        "# kernel doesn't install its driver via APT, and as a result the ICD is missing.\n",
        "# (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n",
        "NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
        "if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
        "  with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
        "    f.write(\"\"\"{\n",
        "    \"file_format_version\" : \"1.0.0\",\n",
        "    \"ICD\" : {\n",
        "        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
        "    }\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "# Tell XLA to use Triton GEMM, this improves steps/sec by ~30% on some GPUs\n",
        "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
        "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
        "os.environ['XLA_FLAGS'] = xla_flags\n",
        "\n",
        "# Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
        "print('Setting environment variable to use GPU rendering:')\n",
        "%env MUJOCO_GL=egl\n",
        "\n",
        "try:\n",
        "  print('Checking that the installation succeeded:')\n",
        "  import mujoco\n",
        "  mujoco.MjModel.from_xml_string('<mujoco/>')\n",
        "except Exception as e:\n",
        "  raise e from RuntimeError(\n",
        "      'Something went wrong during installation. Check the shell output above '\n",
        "      'for more information.\\n'\n",
        "      'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
        "      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
        "\n",
        "print('Installation successful.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5f4w3Kq2X14",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Import packages for plotting and creating graphics\n",
        "import time\n",
        "import itertools\n",
        "import numpy as np\n",
        "from typing import Callable, NamedTuple, Optional, Union, List\n",
        "\n",
        "# Graphics and plotting.\n",
        "print('Installing mediapy:')\n",
        "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
        "!pip install -q mediapy\n",
        "import mediapy as media\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# More legible printing from numpy.\n",
        "np.set_printoptions(precision=3, suppress=True, linewidth=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObF1UXrkb0Nd"
      },
      "outputs": [],
      "source": [
        "#@title Import MuJoCo, MJX, and Brax\n",
        "\n",
        "from datetime import datetime\n",
        "import functools\n",
        "import jax\n",
        "from jax import numpy as jp\n",
        "import numpy as np\n",
        "from typing import Any, Dict, Sequence, Tuple, Union\n",
        "\n",
        "from brax import base\n",
        "from brax import envs\n",
        "from brax import math\n",
        "from brax.base import Base, Motion, Transform\n",
        "from brax.envs.base import Env, PipelineEnv, State\n",
        "from brax.mjx.base import State as MjxState\n",
        "from brax.training.agents.ppo import train as ppo\n",
        "from brax.io import html, mjcf, model\n",
        "\n",
        "from etils import epath\n",
        "from flax import struct\n",
        "from matplotlib import pyplot as plt\n",
        "import mediapy as media\n",
        "from ml_collections import config_dict\n",
        "import mujoco\n",
        "from mujoco import mjx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAv6WUVUm78k"
      },
      "source": [
        "# Training a Policy with MJX\n",
        "MJX is an implementation of MuJoCo written in [JAX](https://jax.readthedocs.io/en/latest/index.html), enabling large batch training on GPU/TPU. In this notebook, we demonstrate how to train RL policies with MJX.\n",
        "\n",
        "`State` holds the observation, reward, metrics, and environment info. Notably `State.pipeline_state` holds a `mjx.Data` object, which is analogous to `mjData` in MuJoCo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPlFu4CiIgBN"
      },
      "source": [
        "Finally we can implement a real environment. We choose to first implement the Humanoid environment. Notice that `reset` initializes a `State`, and `step` steps through the physics step and reward logic. The reward and stepping logic train the Humanoid to run forwards."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Humanoid XML\n",
        "\n",
        "xml = \"\"\"\n",
        "<mujoco model=\"Humanoid and a ball\">\n",
        "  <option timestep=\"0.005\" iterations=\"1\" ls_iterations=\"4\">\n",
        "    <flag eulerdamp=\"disable\"/>\n",
        "  </option>\n",
        "\n",
        "  <visual>\n",
        "    <map force=\"0.1\" zfar=\"30\" znear=\"0.1\" />\n",
        "    <rgba haze=\"0.15 0.25 0.35 1\"/>\n",
        "    <global offwidth=\"2560\" offheight=\"1440\" elevation=\"-20\" azimuth=\"120\"/>\n",
        "    <quality shadowsize=\"4096\" offsamples=\"8\"/>\n",
        "  </visual>\n",
        "\n",
        "  <statistic center=\"0 0 0.7\" extent=\"4\"/>\n",
        "\n",
        "  <asset>\n",
        "    <texture type=\"skybox\" builtin=\"gradient\" rgb1=\".3 .5 .7\" rgb2=\"0 0 0\" width=\"32\" height=\"512\"/>\n",
        "    <texture name=\"body\" type=\"cube\" builtin=\"flat\" mark=\"cross\" width=\"128\" height=\"128\" rgb1=\"0.8 0.6 0.4\" rgb2=\"0.8 0.6 0.4\" markrgb=\"1 1 1\" random=\"0.01\"/>\n",
        "    <material name=\"body\" texture=\"body\" texuniform=\"true\" rgba=\"0.8 0.6 .4 1\"/>\n",
        "\n",
        "    <texture type=\"skybox\" builtin=\"gradient\" width=\"512\" height=\"512\" rgb1=\".4 .6 .8\" rgb2=\"0 0 0\"/>\n",
        "    <texture name=\"texgeom\" type=\"cube\" builtin=\"flat\" mark=\"cross\" width=\"128\" height=\"128\"\n",
        "             rgb1=\"0.6 0.6 0.6\" rgb2=\"0.6 0.6 0.6\" markrgb=\"1 1 1\"/>\n",
        "    <texture name=\"texplane\" type=\"2d\" builtin=\"checker\" rgb1=\".4 .4 .4\" rgb2=\".6 .6 .6\"\n",
        "             width=\"512\" height=\"512\"/>\n",
        "    <material name='MatPlane' reflectance='0.3' texture=\"texplane\" texrepeat=\"1 1\" texuniform=\"true\"\n",
        "              rgba=\".7 .7 .7 1\"/>\n",
        "    <material name='ball' texture=\"texgeom\" texuniform=\"true\" rgba=\".9 .1 .1 1\" />\n",
        "  </asset>\n",
        "\n",
        "  <default>\n",
        "    <default class=\"ball\">\n",
        "      <geom type=\"sphere\" material=\"ball\" size=\"0.15\" mass=\"0.045\" friction=\"0.7 0.075 0.075\" solref=\"0.02 1.0\"/>\n",
        "    </default>\n",
        "    <motor ctrlrange=\"-1 1\" ctrllimited=\"true\"/>\n",
        "    <default class=\"body\">\n",
        "\n",
        "      <!-- geoms -->\n",
        "      <geom type=\"capsule\" condim=\"3\" friction=\".7\" solimp=\".9 .99 .003\" solref=\".015 1\" material=\"body\" contype=\"0\" conaffinity=\"0\"/>\n",
        "      <default class=\"thigh\">\n",
        "        <geom size=\".06\"/>\n",
        "      </default>\n",
        "      <default class=\"shin\">\n",
        "        <geom fromto=\"0 0 0 0 0 -.3\"  size=\".049\"/>\n",
        "      </default>\n",
        "      <default class=\"foot\">\n",
        "        <geom size=\".027\"/>\n",
        "        <default class=\"foot1\">\n",
        "          <geom fromto=\"-.07 -.01 0 .14 -.03 0\"/>\n",
        "        </default>\n",
        "        <default class=\"foot2\">\n",
        "          <geom fromto=\"-.07 .01 0 .14  .03 0\"/>\n",
        "        </default>\n",
        "      </default>\n",
        "      <default class=\"arm_upper\">\n",
        "        <geom size=\".04\"/>\n",
        "      </default>\n",
        "      <default class=\"arm_lower\">\n",
        "        <geom size=\".031\"/>\n",
        "      </default>\n",
        "      <default class=\"hand\">\n",
        "        <geom type=\"sphere\" size=\".04\"/>\n",
        "      </default>\n",
        "\n",
        "      <!-- joints -->\n",
        "      <joint type=\"hinge\" damping=\".2\" stiffness=\"1\" armature=\".01\" limited=\"true\" solimplimit=\"0 .99 .01\"/>\n",
        "      <default class=\"joint_big\">\n",
        "        <joint damping=\"5\" stiffness=\"10\"/>\n",
        "        <default class=\"hip_x\">\n",
        "          <joint range=\"-30 10\"/>\n",
        "        </default>\n",
        "        <default class=\"hip_z\">\n",
        "          <joint range=\"-60 35\"/>\n",
        "        </default>\n",
        "        <default class=\"hip_y\">\n",
        "          <joint axis=\"0 1 0\" range=\"-150 20\"/>\n",
        "        </default>\n",
        "        <default class=\"joint_big_stiff\">\n",
        "          <joint stiffness=\"20\"/>\n",
        "        </default>\n",
        "      </default>\n",
        "      <default class=\"knee\">\n",
        "        <joint pos=\"0 0 .02\" axis=\"0 -1 0\" range=\"-160 2\"/>\n",
        "      </default>\n",
        "      <default class=\"ankle\">\n",
        "        <joint range=\"-50 50\"/>\n",
        "        <default class=\"ankle_y\">\n",
        "          <joint pos=\"0 0 .08\" axis=\"0 1 0\" stiffness=\"6\"/>\n",
        "        </default>\n",
        "        <default class=\"ankle_x\">\n",
        "          <joint pos=\"0 0 .04\" stiffness=\"3\"/>\n",
        "        </default>\n",
        "      </default>\n",
        "      <default class=\"shoulder\">\n",
        "        <joint range=\"-85 60\"/>\n",
        "      </default>\n",
        "      <default class=\"elbow\">\n",
        "        <joint range=\"-100 50\" stiffness=\"0\"/>\n",
        "      </default>\n",
        "    </default>\n",
        "  </default>\n",
        "\n",
        "  <worldbody>\n",
        "    <light directional=\"true\" diffuse=\".8 .8 .8\" pos=\"0 0 10\" dir=\"0 0 -10\"/>\n",
        "    <geom name=\"floor\" type=\"plane\" size=\"10 10 .5\" material=\"MatPlane\" condim=\"3\"/>\n",
        "\n",
        "    <body name=\"torso\" pos=\"0 0 1.282\" childclass=\"body\">\n",
        "      <light name=\"top\" pos=\"0 0 2\" mode=\"trackcom\"/>\n",
        "      # <camera name=\"back\" pos=\"-3 0 1\" xyaxes=\"0 -1 0 1 0 2\" mode=\"trackcom\"/>\n",
        "      <camera name=\"back\" pos=\"-6 0 1\" xyaxes=\"0 -1 0 1 0 2\" mode=\"trackcom\"/>\n",
        "      # <camera name=\"side\" pos=\"0 -3 1\" xyaxes=\"1 0 0 0 1 2\" mode=\"trackcom\"/>\n",
        "      <camera name=\"side\" pos=\"0 -6 1\" xyaxes=\"1 0 0 0 1 2\" mode=\"trackcom\"/>\n",
        "      <freejoint name=\"root\"/>\n",
        "      <geom name=\"torso\" fromto=\"0 -.07 0 0 .07 0\" size=\".07\"/>\n",
        "      <geom name=\"waist_upper\" fromto=\"-.01 -.06 -.12 -.01 .06 -.12\" size=\".06\"/>\n",
        "      <body name=\"head\" pos=\"0 0 .19\">\n",
        "        # <geom name=\"head\" type=\"sphere\" size=\".09\"/>\n",
        "        <geom name=\"head\" type=\"cylinder\" size=\".09 .18\"/>\n",
        "        <camera name=\"egocentric\" pos=\".09 0 0\" xyaxes=\"0 -1 0 .1 0 1\" fovy=\"80\"/>\n",
        "      </body>\n",
        "      <body name=\"waist_lower\" pos=\"-.01 0 -.26\">\n",
        "        <geom name=\"waist_lower\" fromto=\"0 -.06 0 0 .06 0\" size=\".06\"/>\n",
        "        <joint name=\"abdomen_z\" pos=\"0 0 .065\" axis=\"0 0 1\" range=\"-45 45\" class=\"joint_big_stiff\"/>\n",
        "        <joint name=\"abdomen_y\" pos=\"0 0 .065\" axis=\"0 1 0\" range=\"-75 30\" class=\"joint_big\"/>\n",
        "        <body name=\"pelvis\" pos=\"0 0 -.165\">\n",
        "          <joint name=\"abdomen_x\" pos=\"0 0 .1\" axis=\"1 0 0\" range=\"-35 35\" class=\"joint_big\"/>\n",
        "          <geom name=\"butt\" fromto=\"-.02 -.07 0 -.02 .07 0\" size=\".09\"/>\n",
        "          <body name=\"thigh_right\" pos=\"0 -.1 -.04\">\n",
        "            <joint name=\"hip_x_right\" axis=\"1 0 0\" class=\"hip_x\"/>\n",
        "            <joint name=\"hip_z_right\" axis=\"0 0 1\" class=\"hip_z\"/>\n",
        "            <joint name=\"hip_y_right\" class=\"hip_y\"/>\n",
        "            <geom name=\"thigh_right\" fromto=\"0 0 0 0 .01 -.34\" class=\"thigh\"/>\n",
        "            <body name=\"shin_right\" pos=\"0 .01 -.4\">\n",
        "              <joint name=\"knee_right\" class=\"knee\"/>\n",
        "              <geom name=\"shin_right\" class=\"shin\"/>\n",
        "              <body name=\"foot_right\" pos=\"0 0 -.39\">\n",
        "                <joint name=\"ankle_y_right\" class=\"ankle_y\"/>\n",
        "                <joint name=\"ankle_x_right\" class=\"ankle_x\" axis=\"1 0 .5\"/>\n",
        "                <geom name=\"foot1_right\" class=\"foot1\"/>\n",
        "                <geom name=\"foot2_right\" class=\"foot2\"/>\n",
        "              </body>\n",
        "            </body>\n",
        "          </body>\n",
        "          <body name=\"thigh_left\" pos=\"0 .1 -.04\">\n",
        "            <joint name=\"hip_x_left\" axis=\"-1 0 0\" class=\"hip_x\"/>\n",
        "            <joint name=\"hip_z_left\" axis=\"0 0 -1\" class=\"hip_z\"/>\n",
        "            <joint name=\"hip_y_left\" class=\"hip_y\"/>\n",
        "            <geom name=\"thigh_left\" fromto=\"0 0 0 0 -.01 -.34\" class=\"thigh\"/>\n",
        "            <body name=\"shin_left\" pos=\"0 -.01 -.4\">\n",
        "              <joint name=\"knee_left\" class=\"knee\"/>\n",
        "              <geom name=\"shin_left\" fromto=\"0 0 0 0 0 -.3\" class=\"shin\"/>\n",
        "              <body name=\"foot_left\" pos=\"0 0 -.39\">\n",
        "                <joint name=\"ankle_y_left\" class=\"ankle_y\"/>\n",
        "                <joint name=\"ankle_x_left\" class=\"ankle_x\" axis=\"-1 0 -.5\"/>\n",
        "                <geom name=\"foot1_left\" class=\"foot1\"/>\n",
        "                <geom name=\"foot2_left\" class=\"foot2\"/>\n",
        "              </body>\n",
        "            </body>\n",
        "          </body>\n",
        "        </body>\n",
        "      </body>\n",
        "      <body name=\"upper_arm_right\" pos=\"0 -.17 .06\">\n",
        "        <joint name=\"shoulder1_right\" axis=\"2 1 1\"  class=\"shoulder\"/>\n",
        "        <joint name=\"shoulder2_right\" axis=\"0 -1 1\" class=\"shoulder\"/>\n",
        "        <geom name=\"upper_arm_right\" fromto=\"0 0 0 .16 -.16 -.16\" class=\"arm_upper\"/>\n",
        "        <body name=\"lower_arm_right\" pos=\".18 -.18 -.18\">\n",
        "          <joint name=\"elbow_right\" axis=\"0 -1 1\" class=\"elbow\"/>\n",
        "          <geom name=\"lower_arm_right\" fromto=\".01 .01 .01 .17 .17 .17\" class=\"arm_lower\"/>\n",
        "          <body name=\"hand_right\" pos=\".18 .18 .18\">\n",
        "            <geom name=\"hand_right\" zaxis=\"1 1 1\" class=\"hand\"/>\n",
        "          </body>\n",
        "        </body>\n",
        "      </body>\n",
        "      <body name=\"upper_arm_left\" pos=\"0 .17 .06\">\n",
        "        <joint name=\"shoulder1_left\" axis=\"-2 1 -1\" class=\"shoulder\"/>\n",
        "        <joint name=\"shoulder2_left\" axis=\"0 -1 -1\"  class=\"shoulder\"/>\n",
        "        <geom name=\"upper_arm_left\" fromto=\"0 0 0 .16 .16 -.16\" class=\"arm_upper\"/>\n",
        "        <body name=\"lower_arm_left\" pos=\".18 .18 -.18\">\n",
        "          <joint name=\"elbow_left\" axis=\"0 -1 -1\" class=\"elbow\"/>\n",
        "          <geom name=\"lower_arm_left\" fromto=\".01 -.01 .01 .17 -.17 .17\" class=\"arm_lower\"/>\n",
        "          <body name=\"hand_left\" pos=\".18 -.18 .18\">\n",
        "            <geom name=\"hand_left\" zaxis=\"1 -1 1\" class=\"hand\"/>\n",
        "          </body>\n",
        "        </body>\n",
        "      </body>\n",
        "    </body>\n",
        "\n",
        "    <body name=\"ball\" pos=\"0 0 1.85\" quat=\"0.632456 -0.632456 0.316228 0.316228\">\n",
        "      <freejoint/>\n",
        "      <geom class=\"ball\" name=\"ball\"/>\n",
        "    </body>\n",
        "  </worldbody>\n",
        "\n",
        "  <contact>\n",
        "    <exclude body1=\"waist_lower\" body2=\"thigh_right\"/>\n",
        "    <exclude body1=\"waist_lower\" body2=\"thigh_left\"/>\n",
        "    <pair geom1=\"foot1_left\" geom2=\"floor\"/>\n",
        "    <pair geom1=\"foot1_right\" geom2=\"floor\"/>\n",
        "    <pair geom1=\"foot2_left\" geom2=\"floor\"/>\n",
        "    <pair geom1=\"foot2_right\" geom2=\"floor\"/>\n",
        "    <pair geom1=\"head\" geom2=\"ball\"/>\n",
        "    <pair geom1=\"foot1_left\" geom2=\"ball\"/>\n",
        "    <pair geom1=\"foot1_right\" geom2=\"ball\"/>\n",
        "    <pair geom1=\"foot2_left\" geom2=\"ball\"/>\n",
        "    <pair geom1=\"foot2_right\" geom2=\"ball\"/>\n",
        "    <pair geom1=\"shin_left\" geom2=\"ball\"/>\n",
        "    <pair geom1=\"shin_right\" geom2=\"ball\"/>\n",
        "    <pair geom1=\"thigh_left\" geom2=\"ball\"/>\n",
        "    <pair geom1=\"thigh_right\" geom2=\"ball\"/>\n",
        "  </contact>\n",
        "\n",
        "  <actuator>\n",
        "    <motor name=\"abdomen_y\"       gear=\"40\"  joint=\"abdomen_y\"/>\n",
        "    <motor name=\"abdomen_z\"       gear=\"40\"  joint=\"abdomen_z\"/>\n",
        "    <motor name=\"abdomen_x\"       gear=\"40\"  joint=\"abdomen_x\"/>\n",
        "    <motor name=\"hip_x_right\"     gear=\"40\"  joint=\"hip_x_right\"/>\n",
        "    <motor name=\"hip_z_right\"     gear=\"40\"  joint=\"hip_z_right\"/>\n",
        "    <motor name=\"hip_y_right\"     gear=\"120\" joint=\"hip_y_right\"/>\n",
        "    <motor name=\"knee_right\"      gear=\"80\"  joint=\"knee_right\"/>\n",
        "    <motor name=\"ankle_x_right\"   gear=\"20\"  joint=\"ankle_x_right\"/>\n",
        "    <motor name=\"ankle_y_right\"   gear=\"20\"  joint=\"ankle_y_right\"/>\n",
        "    <motor name=\"hip_x_left\"      gear=\"40\"  joint=\"hip_x_left\"/>\n",
        "    <motor name=\"hip_z_left\"      gear=\"40\"  joint=\"hip_z_left\"/>\n",
        "    <motor name=\"hip_y_left\"      gear=\"120\" joint=\"hip_y_left\"/>\n",
        "    <motor name=\"knee_left\"       gear=\"80\"  joint=\"knee_left\"/>\n",
        "    <motor name=\"ankle_x_left\"    gear=\"20\"  joint=\"ankle_x_left\"/>\n",
        "    <motor name=\"ankle_y_left\"    gear=\"20\"  joint=\"ankle_y_left\"/>\n",
        "    <motor name=\"shoulder1_right\" gear=\"20\"  joint=\"shoulder1_right\"/>\n",
        "    <motor name=\"shoulder2_right\" gear=\"20\"  joint=\"shoulder2_right\"/>\n",
        "    <motor name=\"elbow_right\"     gear=\"40\"  joint=\"elbow_right\"/>\n",
        "    <motor name=\"shoulder1_left\"  gear=\"20\"  joint=\"shoulder1_left\"/>\n",
        "    <motor name=\"shoulder2_left\"  gear=\"20\"  joint=\"shoulder2_left\"/>\n",
        "    <motor name=\"elbow_left\"      gear=\"40\"  joint=\"elbow_left\"/>\n",
        "  </actuator>\n",
        "</mujoco>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "G5RKmMLkUuC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Debug\n",
        "if debug:\n",
        "  model_debug = mujoco.MjModel.from_xml_string(xml)\n",
        "  print(model_debug.nbody)\n",
        "  data = mujoco.MjData(model_debug)\n",
        "  len(data.subtree_com)\n",
        "  print(data.body(\"ball\").subtree_com)\n",
        "  data.body(\"ball\")\n",
        "  print(data.body(0))\n",
        "  print(data.body(17))\n",
        "  print(data.qpos[30])\n",
        "  print(len(data.qpos))"
      ],
      "metadata": {
        "id": "PV5XtnYo--TU",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtGMYNLE3QJN"
      },
      "outputs": [],
      "source": [
        "#@title Humanoid Env\n",
        "\n",
        "class Humanoid(PipelineEnv):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      ball_reward=500.0,\n",
        "      ctrl_cost_weight=0.1,\n",
        "      healthy_reward=1.0,\n",
        "      terminate_when_unhealthy=True,\n",
        "      healthy_z_range=(0.5, 2.0),\n",
        "      # ball_healthy_x_y_range=(-0.5, 0.5),\n",
        "      # ball_healthy_z_range=(1.0, 2.4),\n",
        "      ball_healthy_min_z=1.0,\n",
        "      # ball_reward_min_z=2.0,\n",
        "      ball_reward_z=2.0,\n",
        "      reset_noise_scale=1e-2,\n",
        "      exclude_current_positions_from_observation=True,\n",
        "      **kwargs,\n",
        "  ):\n",
        "    mj_model = mujoco.MjModel.from_xml_string(xml)\n",
        "    mj_model.opt.solver = mujoco.mjtSolver.mjSOL_CG\n",
        "    mj_model.opt.iterations = 6\n",
        "    mj_model.opt.ls_iterations = 6\n",
        "\n",
        "    sys = mjcf.load_model(mj_model)\n",
        "\n",
        "    physics_steps_per_control_step = 5\n",
        "    kwargs['n_frames'] = kwargs.get(\n",
        "        'n_frames', physics_steps_per_control_step)\n",
        "    kwargs['backend'] = 'mjx'\n",
        "\n",
        "    super().__init__(sys, **kwargs)\n",
        "\n",
        "    self._ball_reward = ball_reward\n",
        "    self._ctrl_cost_weight = ctrl_cost_weight\n",
        "    self._healthy_reward = healthy_reward\n",
        "    self._terminate_when_unhealthy = terminate_when_unhealthy\n",
        "    self._healthy_z_range = healthy_z_range\n",
        "    # self._ball_healthy_x_y_range = ball_healthy_x_y_range\n",
        "    # self._ball_healthy_z_range = ball_healthy_z_range\n",
        "    self._ball_healthy_min_z = ball_healthy_min_z\n",
        "    # self._ball_reward_min_z = ball_reward_min_z\n",
        "    self._ball_reward_z = ball_reward_z\n",
        "    self._reset_noise_scale = reset_noise_scale\n",
        "    self._exclude_current_positions_from_observation = (\n",
        "        exclude_current_positions_from_observation\n",
        "    )\n",
        "\n",
        "  def reset(self, rng: jp.ndarray) -> State:\n",
        "    \"\"\"Resets the environment to an initial state.\"\"\"\n",
        "    rng, rng1, rng2 = jax.random.split(rng, 3)\n",
        "\n",
        "    low, hi = -self._reset_noise_scale, self._reset_noise_scale\n",
        "    qpos = self.sys.qpos0 + jax.random.uniform(\n",
        "        rng1, (self.sys.nq,), minval=low, maxval=hi\n",
        "    )\n",
        "    qvel = jax.random.uniform(\n",
        "        rng2, (self.sys.nv,), minval=low, maxval=hi\n",
        "    )\n",
        "\n",
        "    data = self.pipeline_init(qpos, qvel)\n",
        "\n",
        "    obs = self._get_obs(data, jp.zeros(self.sys.nu))\n",
        "    reward, done, zero = jp.zeros(3)\n",
        "    metrics = {\n",
        "        'ball_reward': zero,\n",
        "        'reward_quadctrl': zero,\n",
        "        'reward_alive': zero,\n",
        "        'reward': zero,\n",
        "    }\n",
        "    return State(data, obs, reward, done, metrics)\n",
        "\n",
        "  def step(self, state: State, action: jp.ndarray) -> State:\n",
        "    \"\"\"Runs one timestep of the environment's dynamics.\"\"\"\n",
        "    data0 = state.pipeline_state\n",
        "    data = self.pipeline_step(data0, action)\n",
        "\n",
        "    com_before_ball = data0.subtree_com[-1]\n",
        "    com_after_ball = data.subtree_com[-1]\n",
        "\n",
        "    # velocity_ball = (com_after_ball - com_before_ball) / self.dt\n",
        "\n",
        "    min_z, max_z = self._healthy_z_range\n",
        "    is_healthy = jp.where(data.q[2] < min_z, 0.0, 1.0)\n",
        "    is_healthy = jp.where(data.q[2] > max_z, 0.0, is_healthy)\n",
        "\n",
        "    # ball_min_x, ball_max_x = self._ball_healthy_x_y_range\n",
        "    # ball_min_y, ball_max_y = self._ball_healthy_x_y_range\n",
        "    # ball_min_z, ball_max_z = self._ball_healthy_z_range\n",
        "    # is_healthy = jp.where(com_after_ball[0] < ball_min_x, 0.0, is_healthy)\n",
        "    # is_healthy = jp.where(com_after_ball[0] > ball_max_x, 0.0, is_healthy)\n",
        "    # is_healthy = jp.where(com_after_ball[1] < ball_min_y, 0.0, is_healthy)\n",
        "    # is_healthy = jp.where(com_after_ball[1] > ball_max_y, 0.0, is_healthy)\n",
        "    # is_healthy = jp.where(com_after_ball[2] < ball_min_z, 0.0, is_healthy)\n",
        "    # is_healthy = jp.where(com_after_ball[2] > ball_max_z, 0.0, is_healthy)\n",
        "\n",
        "    is_healthy = jp.where(com_after_ball[2] < self._ball_healthy_min_z, 0.0, is_healthy)\n",
        "\n",
        "    if self._terminate_when_unhealthy:\n",
        "      healthy_reward = self._healthy_reward\n",
        "      ball_reward = self._ball_reward\n",
        "    else:\n",
        "      healthy_reward = self._healthy_reward * is_healthy\n",
        "      ball_reward = self._ball_reward * is_healthy\n",
        "\n",
        "    ctrl_cost = self._ctrl_cost_weight * jp.sum(jp.square(action))\n",
        "\n",
        "    obs = self._get_obs(data, action)\n",
        "\n",
        "    is_ball_reward = jp.where(com_before_ball[2] < self._ball_reward_z, 1.0, 0.0)\n",
        "    is_ball_reward = jp.where(com_after_ball[2] >= self._ball_reward_z, 1.0, is_ball_reward)\n",
        "    reward = ball_reward * is_ball_reward + healthy_reward - ctrl_cost\n",
        "\n",
        "    done = 1.0 - is_healthy if self._terminate_when_unhealthy else 0.0\n",
        "    state.metrics.update(\n",
        "        ball_reward=ball_reward,\n",
        "        reward_quadctrl=-ctrl_cost,\n",
        "        reward_alive=healthy_reward,\n",
        "        reward=reward,\n",
        "    )\n",
        "\n",
        "    return state.replace(\n",
        "        pipeline_state=data, obs=obs, reward=reward, done=done\n",
        "    )\n",
        "\n",
        "  def _get_obs(\n",
        "      self, data: mjx.Data, action: jp.ndarray\n",
        "  ) -> jp.ndarray:\n",
        "    \"\"\"Observes humanoid body and ball (?) position, velocities, and angles.\n",
        "       https://mujoco.readthedocs.io/en/stable/APIreference/APItypes.html#mjdata\n",
        "       mjx/mujoco/mjx/_src/types.py\n",
        "    \"\"\"\n",
        "    # qpos: position / nq: number of generalized coordinates = dim(qpos)\n",
        "    position = data.qpos\n",
        "    if self._exclude_current_positions_from_observation:\n",
        "      position = position[2:]\n",
        "\n",
        "    # external_contact_forces are excluded\n",
        "    return jp.concatenate([\n",
        "        # TODO: confirm the position of the ball is part of the obs\n",
        "        # qpos: position / nq: number of generalized coordinates = dim(qpos)\n",
        "        position,\n",
        "        # qvel: velocity / nv: number of degrees of freedom = dim(qvel)\n",
        "        data.qvel,\n",
        "        # cinert: com-based body inertia and mass / (nbody, 10)\n",
        "        data.cinert[1:].ravel(),\n",
        "        # cvel: com-based velocity [3D rot; 3D tran] / (nbody, 6)\n",
        "        data.cvel[1:].ravel(),\n",
        "        # qfrc_actuator: actuator force / nv: number of degrees of freedom\n",
        "        data.qfrc_actuator,\n",
        "    ])\n",
        "\n",
        "\n",
        "envs.register_environment('humanoid', Humanoid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1K6IznI2y83"
      },
      "source": [
        "## Visualize a Rollout\n",
        "\n",
        "Let's instantiate the environment and visualize a short rollout.\n",
        "\n",
        "NOTE: Since episodes terminates early if the torso is below the healthy z-range, the only relevant contacts for this task are between the feet and the plane. We turn off other contacts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhKLFK54C1CH"
      },
      "outputs": [],
      "source": [
        "# instantiate the environment\n",
        "env_name = 'humanoid'\n",
        "env = envs.get_environment(env_name)\n",
        "\n",
        "# define the jit reset/step functions\n",
        "jit_reset = jax.jit(env.reset)\n",
        "jit_step = jax.jit(env.step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ph8u-v2Q2xLS"
      },
      "outputs": [],
      "source": [
        "# initialize the state\n",
        "state = jit_reset(jax.random.PRNGKey(0))\n",
        "rollout = [state.pipeline_state]\n",
        "\n",
        "# grab a trajectory\n",
        "for i in range(50):\n",
        "  ctrl = -0.1 * jp.ones(env.sys.nu)\n",
        "  state = jit_step(state, ctrl)\n",
        "  rollout.append(state.pipeline_state)\n",
        "\n",
        "media.show_video(env.render(rollout, camera='side'), fps=1.0 / env.dt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQDG6NQ1CbZD"
      },
      "source": [
        "## Train Humanoid Policy\n",
        "\n",
        "Let's finally train a policy with PPO to make the Humanoid run forwards. Training takes about 13-14 minutes on a Tesla V100 GPU (and 25-30min with a T4 GPU)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define Training Function\n",
        "\n",
        "\"\"\"Proximal policy optimization training.\n",
        "\n",
        "See: https://arxiv.org/pdf/1707.06347.pdf\n",
        "\"\"\"\n",
        "\n",
        "import functools\n",
        "import time\n",
        "from typing import Callable, Optional, Tuple, Union\n",
        "\n",
        "from absl import logging\n",
        "from brax import base\n",
        "from brax import envs\n",
        "from brax.training import acting\n",
        "from brax.training import gradients\n",
        "from brax.training import pmap\n",
        "from brax.training import types\n",
        "from brax.training.acme import running_statistics\n",
        "from brax.training.acme import specs\n",
        "from brax.training.agents.ppo import losses as ppo_losses\n",
        "from brax.training.agents.ppo import networks as ppo_networks\n",
        "from brax.training.types import Params, PolicyParams, PreprocessorParams\n",
        "from brax.training.types import PRNGKey\n",
        "from brax.v1 import envs as envs_v1\n",
        "import flax\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "\n",
        "\n",
        "InferenceParams = Tuple[running_statistics.NestedMeanStd, Params]\n",
        "Metrics = types.Metrics\n",
        "ValueParams = Any\n",
        "\n",
        "_PMAP_AXIS_NAME = 'i'\n",
        "\n",
        "\n",
        "@flax.struct.dataclass\n",
        "class TrainingState:\n",
        "  \"\"\"Contains training state for the learner.\"\"\"\n",
        "  optimizer_state: optax.OptState\n",
        "  params: ppo_losses.PPONetworkParams\n",
        "  normalizer_params: running_statistics.RunningStatisticsState\n",
        "  env_steps: jnp.ndarray\n",
        "\n",
        "\n",
        "def _unpmap(v):\n",
        "  return jax.tree_util.tree_map(lambda x: x[0], v)\n",
        "\n",
        "\n",
        "def _strip_weak_type(tree):\n",
        "  # brax user code is sometimes ambiguous about weak_type.  in order to\n",
        "  # avoid extra jit recompilations we strip all weak types from user input\n",
        "  def f(leaf):\n",
        "    leaf = jnp.asarray(leaf)\n",
        "    return leaf.astype(leaf.dtype)\n",
        "  return jax.tree_util.tree_map(f, tree)\n",
        "\n",
        "\n",
        "def train(\n",
        "    environment: Union[envs_v1.Env, envs.Env],\n",
        "    num_timesteps: int,\n",
        "    episode_length: int,\n",
        "    action_repeat: int = 1,\n",
        "    num_envs: int = 1,\n",
        "    max_devices_per_host: Optional[int] = None,\n",
        "    num_eval_envs: int = 128,\n",
        "    learning_rate: float = 1e-4,\n",
        "    entropy_cost: float = 1e-4,\n",
        "    discounting: float = 0.9,\n",
        "    seed: int = 0,\n",
        "    unroll_length: int = 10,\n",
        "    batch_size: int = 32,\n",
        "    num_minibatches: int = 16,\n",
        "    num_updates_per_batch: int = 2,\n",
        "    num_evals: int = 1,\n",
        "    num_resets_per_eval: int = 0,\n",
        "    normalize_observations: bool = False,\n",
        "    reward_scaling: float = 1.0,\n",
        "    clipping_epsilon: float = 0.3,\n",
        "    gae_lambda: float = 0.95,\n",
        "    deterministic_eval: bool = False,\n",
        "    network_factory: types.NetworkFactory[\n",
        "        ppo_networks.PPONetworks\n",
        "    ] = ppo_networks.make_ppo_networks,\n",
        "    progress_fn: Callable[[int, Metrics], None] = lambda *args: None,\n",
        "    normalize_advantage: bool = True,\n",
        "    eval_env: Optional[envs.Env] = None,\n",
        "    policy_params_fn: Callable[..., None] = lambda *args: None,\n",
        "    randomization_fn: Optional[\n",
        "        Callable[[base.System, jnp.ndarray], Tuple[base.System, base.System]]\n",
        "    ] = None,\n",
        "    saved_params: Optional[\n",
        "        Tuple[PreprocessorParams, PolicyParams, ValueParams]\n",
        "    ] = None,\n",
        "):\n",
        "  \"\"\"PPO training.\n",
        "\n",
        "  Args:\n",
        "    environment: the environment to train\n",
        "    num_timesteps: the total number of environment steps to use during training\n",
        "    episode_length: the length of an environment episode\n",
        "    action_repeat: the number of timesteps to repeat an action\n",
        "    num_envs: the number of parallel environments to use for rollouts\n",
        "      NOTE: `num_envs` must be divisible by the total number of chips since each\n",
        "        chip gets `num_envs // total_number_of_chips` environments to roll out\n",
        "      NOTE: `batch_size * num_minibatches` must be divisible by `num_envs` since\n",
        "        data generated by `num_envs` parallel envs gets used for gradient\n",
        "        updates over `num_minibatches` of data, where each minibatch has a\n",
        "        leading dimension of `batch_size`\n",
        "    max_devices_per_host: maximum number of chips to use per host process\n",
        "    num_eval_envs: the number of envs to use for evaluation. Each env will run 1\n",
        "      episode, and all envs run in parallel during eval.\n",
        "    learning_rate: learning rate for ppo loss\n",
        "    entropy_cost: entropy reward for ppo loss, higher values increase entropy\n",
        "      of the policy\n",
        "    discounting: discounting rate\n",
        "    seed: random seed\n",
        "    unroll_length: the number of timesteps to unroll in each environment. The\n",
        "      PPO loss is computed over `unroll_length` timesteps\n",
        "    batch_size: the batch size for each minibatch SGD step\n",
        "    num_minibatches: the number of times to run the SGD step, each with a\n",
        "      different minibatch with leading dimension of `batch_size`\n",
        "    num_updates_per_batch: the number of times to run the gradient update over\n",
        "      all minibatches before doing a new environment rollout\n",
        "    num_evals: the number of evals to run during the entire training run.\n",
        "      Increasing the number of evals increases total training time\n",
        "    num_resets_per_eval: the number of environment resets to run between each\n",
        "      eval. The environment resets occur on the host\n",
        "    normalize_observations: whether to normalize observations\n",
        "    reward_scaling: float scaling for reward\n",
        "    clipping_epsilon: clipping epsilon for PPO loss\n",
        "    gae_lambda: General advantage estimation lambda\n",
        "    deterministic_eval: whether to run the eval with a deterministic policy\n",
        "    network_factory: function that generates networks for policy and value\n",
        "      functions\n",
        "    progress_fn: a user-defined callback function for reporting/plotting metrics\n",
        "    normalize_advantage: whether to normalize advantage estimate\n",
        "    eval_env: an optional environment for eval only, defaults to `environment`\n",
        "    policy_params_fn: a user-defined callback function that can be used for\n",
        "      saving policy checkpoints\n",
        "    randomization_fn: a user-defined callback function that generates randomized\n",
        "      environments\n",
        "    saved_params: params to init the training with; includes normalizer_params\n",
        "      and policy and value network params\n",
        "\n",
        "  Returns:\n",
        "    Tuple of (make_policy function, network params, metrics)\n",
        "  \"\"\"\n",
        "  assert batch_size * num_minibatches % num_envs == 0\n",
        "  xt = time.time()\n",
        "\n",
        "  process_count = jax.process_count()\n",
        "  process_id = jax.process_index()\n",
        "  local_device_count = jax.local_device_count()\n",
        "  local_devices_to_use = local_device_count\n",
        "  if max_devices_per_host:\n",
        "    local_devices_to_use = min(local_devices_to_use, max_devices_per_host)\n",
        "  logging.info(\n",
        "      'Device count: %d, process count: %d (id %d), local device count: %d, '\n",
        "      'devices to be used count: %d', jax.device_count(), process_count,\n",
        "      process_id, local_device_count, local_devices_to_use)\n",
        "  device_count = local_devices_to_use * process_count\n",
        "\n",
        "  # The number of environment steps executed for every training step.\n",
        "  env_step_per_training_step = (\n",
        "      batch_size * unroll_length * num_minibatches * action_repeat)\n",
        "  num_evals_after_init = max(num_evals - 1, 1)\n",
        "  # The number of training_step calls per training_epoch call.\n",
        "  # equals to ceil(num_timesteps / (num_evals * env_step_per_training_step *\n",
        "  #                                 num_resets_per_eval))\n",
        "  num_training_steps_per_epoch = np.ceil(\n",
        "      num_timesteps\n",
        "      / (\n",
        "          num_evals_after_init\n",
        "          * env_step_per_training_step\n",
        "          * max(num_resets_per_eval, 1)\n",
        "      )\n",
        "  ).astype(int)\n",
        "\n",
        "  key = jax.random.PRNGKey(seed)\n",
        "  global_key, local_key = jax.random.split(key)\n",
        "  del key\n",
        "  local_key = jax.random.fold_in(local_key, process_id)\n",
        "  local_key, key_env, eval_key = jax.random.split(local_key, 3)\n",
        "  # key_networks should be global, so that networks are initialized the same\n",
        "  # way for different processes.\n",
        "  key_policy, key_value = jax.random.split(global_key)\n",
        "  del global_key\n",
        "\n",
        "  assert num_envs % device_count == 0\n",
        "\n",
        "  v_randomization_fn = None\n",
        "  if randomization_fn is not None:\n",
        "    randomization_batch_size = num_envs // local_device_count\n",
        "    # all devices gets the same randomization rng\n",
        "    randomization_rng = jax.random.split(key_env, randomization_batch_size)\n",
        "    v_randomization_fn = functools.partial(\n",
        "        randomization_fn, rng=randomization_rng\n",
        "    )\n",
        "\n",
        "  if isinstance(environment, envs.Env):\n",
        "    wrap_for_training = envs.training.wrap\n",
        "  else:\n",
        "    wrap_for_training = envs_v1.wrappers.wrap_for_training\n",
        "\n",
        "  env = wrap_for_training(\n",
        "      environment,\n",
        "      episode_length=episode_length,\n",
        "      action_repeat=action_repeat,\n",
        "      randomization_fn=v_randomization_fn,\n",
        "  )\n",
        "\n",
        "  reset_fn = jax.jit(jax.vmap(env.reset))\n",
        "  key_envs = jax.random.split(key_env, num_envs // process_count)\n",
        "  key_envs = jnp.reshape(key_envs,\n",
        "                         (local_devices_to_use, -1) + key_envs.shape[1:])\n",
        "  env_state = reset_fn(key_envs)\n",
        "\n",
        "  normalize = lambda x, y: x\n",
        "  if normalize_observations:\n",
        "    normalize = running_statistics.normalize\n",
        "  ppo_network = network_factory(\n",
        "      env_state.obs.shape[-1],\n",
        "      env.action_size,\n",
        "      preprocess_observations_fn=normalize)\n",
        "  make_policy = ppo_networks.make_inference_fn(ppo_network)\n",
        "\n",
        "  optimizer = optax.adam(learning_rate=learning_rate)\n",
        "\n",
        "  loss_fn = functools.partial(\n",
        "      ppo_losses.compute_ppo_loss,\n",
        "      ppo_network=ppo_network,\n",
        "      entropy_cost=entropy_cost,\n",
        "      discounting=discounting,\n",
        "      reward_scaling=reward_scaling,\n",
        "      gae_lambda=gae_lambda,\n",
        "      clipping_epsilon=clipping_epsilon,\n",
        "      normalize_advantage=normalize_advantage)\n",
        "\n",
        "  gradient_update_fn = gradients.gradient_update_fn(\n",
        "      loss_fn, optimizer, pmap_axis_name=_PMAP_AXIS_NAME, has_aux=True)\n",
        "\n",
        "  def minibatch_step(\n",
        "      carry, data: types.Transition,\n",
        "      normalizer_params: running_statistics.RunningStatisticsState):\n",
        "    optimizer_state, params, key = carry\n",
        "    key, key_loss = jax.random.split(key)\n",
        "    (_, metrics), params, optimizer_state = gradient_update_fn(\n",
        "        params,\n",
        "        normalizer_params,\n",
        "        data,\n",
        "        key_loss,\n",
        "        optimizer_state=optimizer_state)\n",
        "\n",
        "    return (optimizer_state, params, key), metrics\n",
        "\n",
        "  def sgd_step(carry, unused_t, data: types.Transition,\n",
        "               normalizer_params: running_statistics.RunningStatisticsState):\n",
        "    optimizer_state, params, key = carry\n",
        "    key, key_perm, key_grad = jax.random.split(key, 3)\n",
        "\n",
        "    def convert_data(x: jnp.ndarray):\n",
        "      x = jax.random.permutation(key_perm, x)\n",
        "      x = jnp.reshape(x, (num_minibatches, -1) + x.shape[1:])\n",
        "      return x\n",
        "\n",
        "    shuffled_data = jax.tree_util.tree_map(convert_data, data)\n",
        "    (optimizer_state, params, _), metrics = jax.lax.scan(\n",
        "        functools.partial(minibatch_step, normalizer_params=normalizer_params),\n",
        "        (optimizer_state, params, key_grad),\n",
        "        shuffled_data,\n",
        "        length=num_minibatches)\n",
        "    return (optimizer_state, params, key), metrics\n",
        "\n",
        "  def training_step(\n",
        "      carry: Tuple[TrainingState, envs.State, PRNGKey],\n",
        "      unused_t) -> Tuple[Tuple[TrainingState, envs.State, PRNGKey], Metrics]:\n",
        "    training_state, state, key = carry\n",
        "    key_sgd, key_generate_unroll, new_key = jax.random.split(key, 3)\n",
        "\n",
        "    policy = make_policy(\n",
        "        (training_state.normalizer_params, training_state.params.policy))\n",
        "\n",
        "    def f(carry, unused_t):\n",
        "      current_state, current_key = carry\n",
        "      current_key, next_key = jax.random.split(current_key)\n",
        "      next_state, data = acting.generate_unroll(\n",
        "          env,\n",
        "          current_state,\n",
        "          policy,\n",
        "          current_key,\n",
        "          unroll_length,\n",
        "          extra_fields=('truncation',))\n",
        "      return (next_state, next_key), data\n",
        "\n",
        "    (state, _), data = jax.lax.scan(\n",
        "        f, (state, key_generate_unroll), (),\n",
        "        length=batch_size * num_minibatches // num_envs)\n",
        "    # Have leading dimensions (batch_size * num_minibatches, unroll_length)\n",
        "    data = jax.tree_util.tree_map(lambda x: jnp.swapaxes(x, 1, 2), data)\n",
        "    data = jax.tree_util.tree_map(lambda x: jnp.reshape(x, (-1,) + x.shape[2:]),\n",
        "                                  data)\n",
        "    assert data.discount.shape[1:] == (unroll_length,)\n",
        "\n",
        "    # Update normalization params and normalize observations.\n",
        "    normalizer_params = running_statistics.update(\n",
        "        training_state.normalizer_params,\n",
        "        data.observation,\n",
        "        pmap_axis_name=_PMAP_AXIS_NAME)\n",
        "\n",
        "    (optimizer_state, params, _), metrics = jax.lax.scan(\n",
        "        functools.partial(\n",
        "            sgd_step, data=data, normalizer_params=normalizer_params),\n",
        "        (training_state.optimizer_state, training_state.params, key_sgd), (),\n",
        "        length=num_updates_per_batch)\n",
        "\n",
        "    new_training_state = TrainingState(\n",
        "        optimizer_state=optimizer_state,\n",
        "        params=params,\n",
        "        normalizer_params=normalizer_params,\n",
        "        env_steps=training_state.env_steps + env_step_per_training_step)\n",
        "    return (new_training_state, state, new_key), metrics\n",
        "\n",
        "  def training_epoch(training_state: TrainingState, state: envs.State,\n",
        "                     key: PRNGKey) -> Tuple[TrainingState, envs.State, Metrics]:\n",
        "    (training_state, state, _), loss_metrics = jax.lax.scan(\n",
        "        training_step, (training_state, state, key), (),\n",
        "        length=num_training_steps_per_epoch)\n",
        "    loss_metrics = jax.tree_util.tree_map(jnp.mean, loss_metrics)\n",
        "    return training_state, state, loss_metrics\n",
        "\n",
        "  training_epoch = jax.pmap(training_epoch, axis_name=_PMAP_AXIS_NAME)\n",
        "\n",
        "  # Note that this is NOT a pure jittable method.\n",
        "  def training_epoch_with_timing(\n",
        "      training_state: TrainingState, env_state: envs.State,\n",
        "      key: PRNGKey) -> Tuple[TrainingState, envs.State, Metrics]:\n",
        "    nonlocal training_walltime\n",
        "    t = time.time()\n",
        "    training_state, env_state = _strip_weak_type((training_state, env_state))\n",
        "    result = training_epoch(training_state, env_state, key)\n",
        "    training_state, env_state, metrics = _strip_weak_type(result)\n",
        "\n",
        "    metrics = jax.tree_util.tree_map(jnp.mean, metrics)\n",
        "    jax.tree_util.tree_map(lambda x: x.block_until_ready(), metrics)\n",
        "\n",
        "    epoch_training_time = time.time() - t\n",
        "    training_walltime += epoch_training_time\n",
        "    sps = (num_training_steps_per_epoch *\n",
        "           env_step_per_training_step *\n",
        "           max(num_resets_per_eval, 1)) / epoch_training_time\n",
        "    metrics = {\n",
        "        'training/sps': sps,\n",
        "        'training/walltime': training_walltime,\n",
        "        **{f'training/{name}': value for name, value in metrics.items()}\n",
        "    }\n",
        "    return training_state, env_state, metrics  # pytype: disable=bad-return-type  # py311-upgrade\n",
        "\n",
        "\n",
        "  if saved_params is None:\n",
        "    init_params = ppo_losses.PPONetworkParams(\n",
        "      policy=ppo_network.policy_network.init(key_policy),\n",
        "      value=ppo_network.value_network.init(key_value))\n",
        "    normalizer_params = running_statistics.init_state(\n",
        "      specs.Array(env_state.obs.shape[-1:], jnp.dtype('float32')))\n",
        "  else:\n",
        "    init_params = ppo_losses.PPONetworkParams(\n",
        "      policy=saved_params[1],\n",
        "      value=saved_params[2])\n",
        "    normalizer_params = saved_params[0]\n",
        "\n",
        "  training_state = TrainingState(  # pytype: disable=wrong-arg-types  # jax-ndarray\n",
        "      optimizer_state=optimizer.init(init_params),  # pytype: disable=wrong-arg-types  # numpy-scalars\n",
        "      params=init_params,\n",
        "      normalizer_params=normalizer_params,\n",
        "      env_steps=0)\n",
        "  training_state = jax.device_put_replicated(\n",
        "      training_state,\n",
        "      jax.local_devices()[:local_devices_to_use])\n",
        "\n",
        "  if not eval_env:\n",
        "    eval_env = environment\n",
        "  if randomization_fn is not None:\n",
        "    v_randomization_fn = functools.partial(\n",
        "        randomization_fn, rng=jax.random.split(eval_key, num_eval_envs)\n",
        "    )\n",
        "  eval_env = wrap_for_training(\n",
        "      eval_env,\n",
        "      episode_length=episode_length,\n",
        "      action_repeat=action_repeat,\n",
        "      randomization_fn=v_randomization_fn,\n",
        "  )\n",
        "\n",
        "  evaluator = acting.Evaluator(\n",
        "      eval_env,\n",
        "      functools.partial(make_policy, deterministic=deterministic_eval),\n",
        "      num_eval_envs=num_eval_envs,\n",
        "      episode_length=episode_length,\n",
        "      action_repeat=action_repeat,\n",
        "      key=eval_key)\n",
        "\n",
        "  # Run initial eval\n",
        "  metrics = {}\n",
        "  if process_id == 0 and num_evals > 1:\n",
        "    metrics = evaluator.run_evaluation(\n",
        "        _unpmap(\n",
        "            (training_state.normalizer_params, training_state.params.policy)),\n",
        "        training_metrics={})\n",
        "    logging.info(metrics)\n",
        "    progress_fn(0, metrics)\n",
        "\n",
        "  training_metrics = {}\n",
        "  training_walltime = 0\n",
        "  current_step = 0\n",
        "  # Initialize variables to allow saving params of run with max score\n",
        "  max_score = 0\n",
        "  max_score_params = {}\n",
        "  for it in range(num_evals_after_init):\n",
        "    logging.info('starting iteration %s %s', it, time.time() - xt)\n",
        "\n",
        "    for _ in range(max(num_resets_per_eval, 1)):\n",
        "      # optimization\n",
        "      epoch_key, local_key = jax.random.split(local_key)\n",
        "      epoch_keys = jax.random.split(epoch_key, local_devices_to_use)\n",
        "      (training_state, env_state, training_metrics) = (\n",
        "          training_epoch_with_timing(training_state, env_state, epoch_keys)\n",
        "      )\n",
        "      current_step = int(_unpmap(training_state.env_steps))\n",
        "\n",
        "      key_envs = jax.vmap(\n",
        "          lambda x, s: jax.random.split(x[0], s),\n",
        "          in_axes=(0, None))(key_envs, key_envs.shape[1])\n",
        "      # TODO: move extra reset logic to the AutoResetWrapper.\n",
        "      env_state = reset_fn(key_envs) if num_resets_per_eval > 0 else env_state\n",
        "\n",
        "    if process_id == 0:\n",
        "      # Run evals.\n",
        "      metrics = evaluator.run_evaluation(\n",
        "          _unpmap(\n",
        "              (training_state.normalizer_params, training_state.params.policy)),\n",
        "          training_metrics)\n",
        "      logging.info(metrics)\n",
        "      progress_fn(current_step, metrics)\n",
        "      params = _unpmap(\n",
        "          (training_state.normalizer_params, training_state.params.policy,\n",
        "           training_state.params.value))\n",
        "\n",
        "      # Save params if this is the max score\n",
        "      eval_score = metrics['eval/episode_reward']\n",
        "      if eval_score > max_score:\n",
        "        max_score = eval_score\n",
        "        max_score_params = {\n",
        "            \"score\": max_score,\n",
        "            \"params\": params,\n",
        "        }\n",
        "      policy_params_fn(current_step, make_policy, params)\n",
        "\n",
        "  total_steps = current_step\n",
        "  assert total_steps >= num_timesteps\n",
        "\n",
        "  # If there was no mistakes the training_state should still be identical on all\n",
        "  # devices.\n",
        "  pmap.assert_is_replicated(training_state)\n",
        "  params = _unpmap(\n",
        "      (training_state.normalizer_params, training_state.params.policy,\n",
        "       training_state.params.value))\n",
        "  logging.info('total steps: %s', total_steps)\n",
        "  pmap.synchronize_hosts()\n",
        "  return (make_policy, params, metrics, max_score_params)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fxmLdUcPUMSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Params\n",
        "if upload_model:\n",
        "  saved_params = model.load_params(model_path)\n",
        "else:\n",
        "  saved_params = None"
      ],
      "metadata": {
        "id": "vgeiw_vNjwcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLiddQYPApBw"
      },
      "outputs": [],
      "source": [
        "#@title Train\n",
        "\n",
        "train_fn = functools.partial(\n",
        "    train, num_timesteps=num_timesteps, num_evals=num_evals,\n",
        "    episode_length=1000, normalize_observations=True, action_repeat=1,\n",
        "    unroll_length=10, num_minibatches=32, num_updates_per_batch=8,\n",
        "    discounting=0.97, learning_rate=learning_rate, entropy_cost=1e-3, num_envs=2048,\n",
        "    batch_size=1024, seed=0, saved_params=saved_params)\n",
        "\n",
        "x_data = []\n",
        "y_data = []\n",
        "ydataerr = []\n",
        "times = [datetime.now()]\n",
        "\n",
        "max_y, min_y = 5000, 0\n",
        "def progress(num_steps, metrics):\n",
        "  times.append(datetime.now())\n",
        "  x_data.append(num_steps)\n",
        "  y_data.append(metrics['eval/episode_reward'])\n",
        "  ydataerr.append(metrics['eval/episode_reward_std'])\n",
        "\n",
        "  plt.xlim([0, train_fn.keywords['num_timesteps'] * 1.1])\n",
        "  plt.ylim([min_y, max_y])\n",
        "\n",
        "  plt.xlabel('# environment steps')\n",
        "  plt.ylabel('reward per episode')\n",
        "  plt.title(f'y={y_data[-1]:.3f}')\n",
        "\n",
        "  plt.errorbar(\n",
        "      x_data, y_data, yerr=ydataerr)\n",
        "  plt.show()\n",
        "\n",
        "make_inference_fn, train_params, _, max_score_params = train_fn(\n",
        "    environment=env, progress_fn=progress)\n",
        "\n",
        "print(f'time to jit: {times[1] - times[0]}')\n",
        "print(f'time to train: {times[-1] - times[1]}')\n",
        "print(max_score_params['score'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYIch0HEApBx"
      },
      "source": [
        "## Save and Load Policy\n",
        "\n",
        "We can save and load the policy using the brax model API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8gI6qH6ApBx"
      },
      "outputs": [],
      "source": [
        "#@title Save Model\n",
        "# model.save_params(model_path, train_params)\n",
        "model.save_params(model_path, max_score_params['params'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4reaWgxApBx",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Load Model and Define Inference Function\n",
        "inference_fn = make_inference_fn(model.load_params(model_path)[:2])\n",
        "jit_inference_fn = jax.jit(inference_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G357XIfApBy"
      },
      "source": [
        "## Visualize Policy\n",
        "\n",
        "Finally we can visualize the policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osYasMw4ApBy"
      },
      "outputs": [],
      "source": [
        "eval_env = envs.get_environment(env_name)\n",
        "\n",
        "jit_reset = jax.jit(eval_env.reset)\n",
        "jit_step = jax.jit(eval_env.step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-UhypudApBy"
      },
      "outputs": [],
      "source": [
        "rollouts = []\n",
        "for i in range(num_rollouts):\n",
        "  i += 1000\n",
        "  # initialize the state\n",
        "  rng = jax.random.PRNGKey(i)\n",
        "  state = jit_reset(rng)\n",
        "  rollout = [state.pipeline_state]\n",
        "  total_reward = 0\n",
        "\n",
        "  # grab a trajectory\n",
        "  n_steps = 100\n",
        "  render_every = 2\n",
        "\n",
        "  for i in range(n_steps):\n",
        "    act_rng, rng = jax.random.split(rng)\n",
        "    ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
        "    state = jit_step(state, ctrl)\n",
        "    total_reward += state.metrics[\"reward\"]\n",
        "    rollout.append(state.pipeline_state)\n",
        "\n",
        "    if state.done:\n",
        "      break\n",
        "\n",
        "  if total_reward > 3150:\n",
        "    print(f\"Score {int(total_reward)}\")\n",
        "\n",
        "  if total_reward > score_threshold:\n",
        "    print(f\"Iteration - {i} with score {int(total_reward)}\")\n",
        "    media.show_video(env.render(rollout[::render_every], camera='side'), fps=1.0 / env.dt / render_every)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuClass": "premium",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "YYIch0HEApBx"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}