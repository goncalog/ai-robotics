{"metadata":{"colab":{"gpuClass":"premium","private_outputs":true,"provenance":[],"collapsed_sections":["YvyGCsgSCxHQ","P1K6IznI2y83"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"accelerator":"GPU","language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7850612,"sourceType":"datasetVersion","datasetId":4603848},{"sourceId":8064209,"sourceType":"datasetVersion","datasetId":4724412}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tutorial: Penalties\n\nIn this tutorial you will learn how to use the physics simulator MuJoCo, and Reinforcement Learning to teach two humanoids to take and stop penalties while competing against each other.","metadata":{"id":"Jz-I63-7YgRO"}},{"cell_type":"markdown","source":"## 1 - Kaggle\nIt's recommended to run this notebook on www.kaggle.com where you can use two T4 GPUs for 30h/week for free.\n\nTo import this notebook into Kaggle you need to:\n- Login to your Kaggle account\n- Create a new notebook\n- Click on `File` and then `Import Notebook`\n- Select the tab `GitHub`\n- Search `goncalog/ai-robotics`\n- Select the file `tutorials/penalties.ipynb`\n- Click the `Import` button\n\nTo run this notebook you can either click the `Run All` button or run each cell individually by clicking the `Run current cell` button.","metadata":{}},{"cell_type":"markdown","source":"## 2 - Config\n\nThis is the configuration to run the tutorial, it includes:\n- Training hyperparameters\n- Mujoco environment variables\n- Rendering variables","metadata":{}},{"cell_type":"code","source":"num_timesteps = 20_000_000\nnum_evals = 9\n# num_envs: the number of parallel environments to use for rollouts\nnum_envs = 2048\n\n# learning_rate: learning rate for ppo loss\nlearning_rate = 3e-4\n# discounting: discounting rate\ndiscounting = 0.97\n# episode_length: the length of an environment episode\nepisode_length = 1000\n# normalize_observations: whether to normalize observations\nnormalize_observations = True\n# action_repeat: the number of timesteps to repeat an action\naction_repeat = 1\n# unroll_length: the number of timesteps to unroll in each environment.\n# The PPO loss is computed over `unroll_length` timesteps\nunroll_length = 10\n# entropy_cost: entropy reward for ppo loss, higher values increase entropy of the policy\nentropy_cost = 1e-3\n# batch_size: the batch size for each minibatch SGD step\nbatch_size = 1024\n# num_minibatches: the number of times to run the SGD step,\n# each with a different minibatch with leading dimension of `batch_size`\nnum_minibatches = 32\n# num_updates_per_batch: the number of times to run the gradient update over\n# all minibatches before doing a new environment rollout\nnum_updates_per_batch = 8\n# reward_scaling: float scaling for reward\nreward_scaling = 1\n# clipping_epsilon: clipping epsilon for PPO loss\nclipping_epsilon = 0.3\n# gae_lambda: General advantage estimation lambda\ngae_lambda = 0.95\n# normalize_advantage: whether to normalize advantage estimate\nnormalize_advantage = True\n\npolicy_hidden_layer_sizes = (32,) * 4\nvalue_hidden_layer_sizes = (256,) * 5\n\ntraining_agent = \"striker\" # can be striker or keeper\nball_size = 0.15\nball_x = 0.3 # x coordinate of centre of mass\ngoal_width = 7.32 / 2 # 7.32m\ngoal_distance = 11 / 2 # 11m\ngoal_height = 2\npost1_y = - goal_width / 2\npost2_y = goal_width / 2\ntorso_index = 2 # index of torso body in mjx data (it contains the head geom)\n\n# Simulation time step in seconds. \n# This is the single most important parameter affecting the speed-accuracy trade-off \n# which is inherent in every physics simulation. \n# Smaller values result in better accuracy and stability\nmj_model_timestep = 0.004\n\nnum_penalties = 5\ntournament_mode = False","metadata":{"id":"-Xt8DyfJYrbd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3 - Install MuJoCo, MJX, and Brax","metadata":{"id":"YvyGCsgSCxHQ"}},{"cell_type":"code","source":"!pip install mujoco\n!pip install mujoco_mjx\n!pip install brax\n\n# Check if MuJoCo installation was successful\nimport distutils.util\nimport os\nimport subprocess\nif subprocess.run('nvidia-smi').returncode:\n  raise RuntimeError(\n      'Cannot communicate with GPU. '\n      'Make sure you are using a GPU runtime. '\n      'Go to the Runtime menu and select Choose runtime type.')\n\n# Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n# This is usually installed as part of an Nvidia driver package, but this\n# kernel doesn't install its driver via APT, and as a result the ICD is missing.\n# (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\nNVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/'\nNVIDIA_ICD_CONFIG_FILE = '10_nvidia.json'\nif not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n  os.makedirs(NVIDIA_ICD_CONFIG_PATH)\n  file_path = os.path.join(NVIDIA_ICD_CONFIG_PATH, NVIDIA_ICD_CONFIG_FILE)\n  with open(file_path, 'w') as f:\n    f.write(\"\"\"{\n    \"file_format_version\" : \"1.0.0\",\n    \"ICD\" : {\n        \"library_path\" : \"libEGL_nvidia.so.0\"\n    }\n}\n\"\"\")\n\n# Tell XLA to use Triton GEMM, this improves steps/sec by ~30% on some GPUs\nxla_flags = os.environ.get('XLA_FLAGS', '')\nxla_flags += ' --xla_gpu_triton_gemm_any=True'\nos.environ['XLA_FLAGS'] = xla_flags\n\n# Configure MuJoCo to use the EGL rendering backend (requires GPU)\nprint('Setting environment variable to use GPU rendering:')\n%env MUJOCO_GL=egl\n\ntry:\n  print('Checking that the installation succeeded:')\n  import mujoco\n  mujoco.MjModel.from_xml_string('<mujoco/>')\nexcept Exception as e:\n  raise e from RuntimeError(\n      'Something went wrong during installation. Check the shell output above '\n      'for more information.\\n'\n      'If using a hosted runtime, make sure you enable GPU acceleration '\n      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n\nprint('Installation successful.')","metadata":{"id":"IbZxYDxzoz5R","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import packages for plotting and creating graphics\nimport time\nimport itertools\nimport numpy as np\nfrom typing import Callable, NamedTuple, Optional, Union, List\n\n# Graphics and plotting.\nprint('Installing mediapy:')\n!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n!pip install -q mediapy\nimport mediapy as media\nimport matplotlib.pyplot as plt\n\n# More legible printing from numpy.\nnp.set_printoptions(precision=3, suppress=True, linewidth=100)\n\n# Import MuJoCo, MJX, and Brax\nfrom datetime import datetime\nimport functools\nimport jax\nfrom jax import numpy as jp\nimport numpy as np\nfrom typing import Any, Dict, Sequence, Tuple, Union\n\nfrom brax import base\nfrom brax import envs\nfrom brax import math\nfrom brax.base import Base, Motion, Transform\nfrom brax.envs.base import Env, PipelineEnv, State\nfrom brax.mjx.base import State as MjxState\nfrom brax.io import html, mjcf, model\nfrom brax.training import distribution, networks\n\nfrom etils import epath\nfrom flax import linen, struct\nfrom matplotlib import pyplot as plt\nimport mediapy as media\nfrom ml_collections import config_dict\nimport mujoco\nfrom mujoco import mjx","metadata":{"id":"T5f4w3Kq2X14","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4 - Setting up the two Humanoid environment with MJX\n\nMJX is an implementation of MuJoCo written in [JAX](https://jax.readthedocs.io/en/latest/index.html), enabling large batch training on GPU/TPU. In this notebook, we train RL policies with MJX.\n\nHere we implement our environment by adapting the original [Humanoid](https://github.com/google-deepmind/mujoco/blob/546a27ca72397b888e314ee4549bcf12d9fd5957/model/humanoid/humanoid.xml) environment to also include a ball, a goal, and a second Humanoid who acts as a keeper. Notice that `reset` initializes a `State`, and `step` steps through the physics step and reward logic. The reward and stepping logic train the Humanoids to take and stop penalties.\n","metadata":{"id":"RAv6WUVUm78k"}},{"cell_type":"markdown","source":"Finally we can implement a real environment. We choose to first implement the Humanoid environment. Notice that `reset` initializes a `State`, and `step` steps through the physics step and reward logic. The reward and stepping logic train the Humanoid to run forwards.","metadata":{"id":"iPlFu4CiIgBN"}},{"cell_type":"code","source":"# Humanoid XML\ndef get_humanoid(player_type: str) -> str:\n    \"\"\"Returns the humanoid XML.\n\n    Parameters:\n        player_type (str): can be 'striker' or 'keeper'.\n\n    Returns:\n        str: The humanoid XML to generate a MJX model.\n    \"\"\"\n    if player_type == \"striker\":\n        x_pos = 0\n        x_front = 1 # facing positive x\n        cameras = f\"\"\"\n        <camera name=\"back_{player_type}\" pos=\"-4.5 -2 2.5\" xyaxes=\"2 -5 0 1 0 3\" mode=\"fixed\"/>\n        <camera name=\"side_{player_type}\" pos=\"0.5 -4.5 2.5\" xyaxes=\"1 0 0 0 1 3\" mode=\"fixed\"/>\n        \"\"\"\n    elif player_type == \"keeper\":\n        x_pos = goal_distance\n        x_front = -1 # facing negative x\n        cameras = f\"\"\"\n        <camera name=\"side_{player_type}\" pos=\"{goal_distance - 0.5} -4.5 2.5\" xyaxes=\"1 0 0 0 1 3\" mode=\"fixed\"/>\n        \"\"\"\n    else:\n        raise Exception(f\"{player_type=} isn't supported.\")\n    \n    humanoid_xml = f\"\"\"\n    <light name=\"top_{player_type}\" pos=\"0 0 2\" mode=\"trackcom\"/>\n    {cameras}\n    <body name=\"torso_{player_type}\" pos=\"{x_pos + 0} 0 1.282\" childclass=\"body\">\n      <freejoint name=\"root_{player_type}\"/>\n      <geom name=\"torso_{player_type}\" fromto=\"0 -.07 0 0 .07 0\" size=\".07\"/>\n      <geom name=\"waist_upper_{player_type}\" fromto=\"{x_front * -.01} -.06 -.12 {x_front * -.01} .06 -.12\" size=\".06\"/>\n      <body name=\"head_{player_type}\" pos=\"0 0 .19\">\n        <geom name=\"head_{player_type}\" type=\"sphere\" size=\".09\"/>\n        <camera name=\"egocentric_{player_type}\" pos=\".09 0 0\" xyaxes=\"0 -1 0 .1 0 1\" fovy=\"80\"/>\n      </body>\n      <body name=\"waist_lower_{player_type}\" pos=\"{x_front * 0.01} 0 -.26\">\n        <geom name=\"waist_lower_{player_type}\" fromto=\"0 -.06 0 0 .06 0\" size=\".06\"/>\n        <joint name=\"abdomen_z_{player_type}\" pos=\"0 0 .065\" axis=\"0 0 {x_front * 1}\" range=\"-45 45\" class=\"joint_big_stiff\"/>\n        <joint name=\"abdomen_y_{player_type}\" pos=\"0 0 .065\" axis=\"0 {x_front * 1} 0\" range=\"-75 30\" class=\"joint_big\"/>\n        <body name=\"pelvis_{player_type}\" pos=\"0 0 -.165\">\n          <joint name=\"abdomen_x_{player_type}\" pos=\"0 0 .1\" axis=\"1 0 0\" range=\"-35 35\" class=\"joint_big\"/>\n          <geom name=\"butt_{player_type}\" fromto=\"{x_front * -.02} -.07 0 {x_front * -.02} .07 0\" size=\".09\"/>\n          <body name=\"thigh_right_{player_type}\" pos=\"0 -.1 -.04\">\n            <joint name=\"hip_x_right_{player_type}\" axis=\"1 0 0\" class=\"hip_x\"/>\n            <joint name=\"hip_z_right_{player_type}\" axis=\"0 0 {x_front * 1}\" class=\"hip_z\"/>\n            <joint name=\"hip_y_right_{player_type}\" class=\"hip_y\" axis=\"0 {x_front * 1} 0\"/>\n            <geom name=\"thigh_right_{player_type}\" fromto=\"0 0 0 0 .01 -.34\" class=\"thigh\"/>\n            <body name=\"shin_right_{player_type}\" pos=\"0 .01 -.4\">\n              <joint name=\"knee_right_{player_type}\" class=\"knee\" axis=\"0 {x_front * -1} 0\"/>\n              <geom name=\"shin_right_{player_type}\" class=\"shin\"/>\n              <body name=\"foot_right_{player_type}\" pos=\"0 0 -.39\">\n                <joint name=\"ankle_y_right_{player_type}\" class=\"ankle_y\" axis=\"0 {x_front * 1} 0\"/>\n                <joint name=\"ankle_x_right_{player_type}\" class=\"ankle_x\" axis=\"1 0 {x_front * .5}\"/>\n                <geom name=\"foot1_right_{player_type}\" class=\"foot1_{player_type}\"/>\n                <geom name=\"foot2_right_{player_type}\" class=\"foot2_{player_type}\"/>\n              </body>\n            </body>\n          </body>\n          <body name=\"thigh_left_{player_type}\" pos=\"0 .1 -.04\">\n            <joint name=\"hip_x_left_{player_type}\" axis=\"-1 0 0\" class=\"hip_x\"/>\n            <joint name=\"hip_z_left_{player_type}\" axis=\"0 0 {x_front * -1}\" class=\"hip_z\"/>\n            <joint name=\"hip_y_left_{player_type}\" class=\"hip_y\" axis=\"0 {x_front * 1} 0\"/>\n            <geom name=\"thigh_left_{player_type}\" fromto=\"0 0 0 0 -.01 -.34\" class=\"thigh\"/>\n            <body name=\"shin_left_{player_type}\" pos=\"0 -.01 -.4\">\n              <joint name=\"knee_left_{player_type}\" class=\"knee\" axis=\"0 {x_front * -1} 0\"/>\n              <geom name=\"shin_left_{player_type}\" fromto=\"0 0 0 0 0 -.3\" class=\"shin\"/>\n              <body name=\"foot_left_{player_type}\" pos=\"0 0 -.39\">\n                <joint name=\"ankle_y_left_{player_type}\" class=\"ankle_y\" axis=\"0 {x_front * 1} 0\"/>\n                <joint name=\"ankle_x_left_{player_type}\" class=\"ankle_x\" axis=\"-1 0 {x_front * -.5}\"/>\n                <geom name=\"foot1_left_{player_type}\" class=\"foot1_{player_type}\"/>\n                <geom name=\"foot2_left_{player_type}\" class=\"foot2_{player_type}\"/>\n              </body>\n            </body>\n          </body>\n        </body>\n      </body>\n      <body name=\"upper_arm_right_{player_type}\" pos=\"0 -.17 .06\">\n        <joint name=\"shoulder1_right_{player_type}\" axis=\"2 {x_front * 1} {x_front * 1}\"  class=\"shoulder\"/>\n        <joint name=\"shoulder2_right_{player_type}\" axis=\"0 {x_front * -1} {x_front * 1}\" class=\"shoulder\"/>\n        <geom name=\"upper_arm_right_{player_type}\" fromto=\"0 0 0 {x_front * .16} -.16 -.16\" class=\"arm_upper\"/>\n        <body name=\"lower_arm_right_{player_type}\" pos=\"{x_front * .18} -.18 -.18\">\n          <joint name=\"elbow_right_{player_type}\" axis=\"0 {x_front * -1} {x_front * 1}\" class=\"elbow\"/>\n          <geom name=\"lower_arm_right_{player_type}\" fromto=\"{x_front * 0.01} .01 .01 {x_front * 0.17} .17 .17\" class=\"arm_lower\"/>\n          <body name=\"hand_right_{player_type}\" pos=\"{x_front * 0.18} .18 .18\">\n            <geom name=\"hand_right_{player_type}\" zaxis=\"1 {x_front * 1} 1\" class=\"hand\"/>\n          </body>\n        </body>\n      </body>\n      <body name=\"upper_arm_left_{player_type}\" pos=\"0 .17 .06\">\n        <joint name=\"shoulder1_left_{player_type}\" axis=\"-2 {x_front * 1} {x_front * -1}\" class=\"shoulder\"/>\n        <joint name=\"shoulder2_left_{player_type}\" axis=\"0 {x_front * -1} {x_front * -1}\"  class=\"shoulder\"/>\n        <geom name=\"upper_arm_left_{player_type}\" fromto=\"0 0 0 {x_front * 0.16} .16 -.16\" class=\"arm_upper\"/>\n        <body name=\"lower_arm_left_{player_type}\" pos=\"{x_front * 0.18} .18 -.18\">\n          <joint name=\"elbow_left_{player_type}\" axis=\"0 {x_front * -1} {x_front * -1}\" class=\"elbow\"/>\n          <geom name=\"lower_arm_left_{player_type}\" fromto=\"{x_front * 0.01} -.01 .01 {x_front * 0.17} -.17 .17\" class=\"arm_lower\"/>\n          <body name=\"hand_left_{player_type}\" pos=\"{x_front * 0.18} -.18 .18\">\n            <geom name=\"hand_left_{player_type}\" zaxis=\"1 {x_front * -1} 1\" class=\"hand\"/>\n          </body>\n        </body>\n      </body>\n    </body>\n    \"\"\"\n    return humanoid_xml\n\n\ndef get_contacts(player_type: str) -> str:\n    \"\"\"Returns the contacts XML.\n\n    Parameters:\n        player_type (str): can be 'striker' or 'keeper'.\n\n    Returns:\n        str: The contacts XML to generate a MJX model.\n    \"\"\"\n    if player_type == \"striker\":\n        extra_contacts = \"\"\n    elif player_type == \"keeper\":\n        extra_contacts = f\"\"\"\n        <pair geom1=\"torso_{player_type}\" geom2=\"floor\"/>\n        <pair geom1=\"waist_upper_{player_type}\" geom2=\"floor\"/>\n        <pair geom1=\"head_{player_type}\" geom2=\"floor\"/>\n        <pair geom1=\"waist_lower_{player_type}\" geom2=\"floor\"/>\n        <pair geom1=\"butt_{player_type}\" geom2=\"floor\"/>\n        <pair geom1=\"thigh_right_{player_type}\" geom2=\"floor\"/>\n        <pair geom1=\"shin_right_{player_type}\" geom2=\"floor\"/>\n        <pair geom1=\"thigh_left_{player_type}\" geom2=\"floor\"/>\n        <pair geom1=\"shin_left_{player_type}\" geom2=\"floor\"/>\n        <pair geom1=\"upper_arm_right_{player_type}\" geom2=\"floor\"/>\n        <pair geom1=\"lower_arm_right_{player_type}\" geom2=\"floor\"/>\n        <pair geom1=\"hand_right_{player_type}\" geom2=\"floor\"/>\n        <pair geom1=\"upper_arm_left_{player_type}\" geom2=\"floor\"/>\n        <pair geom1=\"lower_arm_left_{player_type}\" geom2=\"floor\"/>\n        <pair geom1=\"hand_left_{player_type}\" geom2=\"floor\"/>\n        \n        <pair geom1=\"torso_{player_type}\" geom2=\"ball\"/>\n        <pair geom1=\"waist_upper_{player_type}\" geom2=\"ball\"/>\n        <pair geom1=\"head_{player_type}\" geom2=\"ball\"/>\n        <pair geom1=\"waist_lower_{player_type}\" geom2=\"ball\"/>\n        <pair geom1=\"butt_{player_type}\" geom2=\"ball\"/>\n        <pair geom1=\"thigh_right_{player_type}\" geom2=\"ball\"/>\n        <pair geom1=\"shin_right_{player_type}\" geom2=\"ball\"/>\n        <pair geom1=\"foot1_right_{player_type}\" geom2=\"ball\"/>\n        <pair geom1=\"foot2_right_{player_type}\" geom2=\"ball\"/>\n        <pair geom1=\"thigh_left_{player_type}\" geom2=\"ball\"/>\n        <pair geom1=\"shin_left_{player_type}\" geom2=\"ball\"/>\n        <pair geom1=\"upper_arm_right_{player_type}\" geom2=\"ball\"/>\n        <pair geom1=\"lower_arm_right_{player_type}\" geom2=\"ball\"/>\n        <pair geom1=\"hand_right_{player_type}\" geom2=\"ball\"/>\n        <pair geom1=\"upper_arm_left_{player_type}\" geom2=\"ball\"/>\n        <pair geom1=\"lower_arm_left_{player_type}\" geom2=\"ball\"/>\n        <pair geom1=\"hand_left_{player_type}\" geom2=\"ball\"/>\n        \"\"\"\n    else:\n        raise Exception(f\"{player_type=} isn't supported.\")\n    \n    contact_xml = f\"\"\"\n    <exclude body1=\"waist_lower_{player_type}\" body2=\"thigh_right_{player_type}\"/>\n    <exclude body1=\"waist_lower_{player_type}\" body2=\"thigh_left_{player_type}\"/>\n    <pair geom1=\"foot1_left_{player_type}\" geom2=\"floor\"/>\n    <pair geom1=\"foot1_right_{player_type}\" geom2=\"floor\"/>\n    <pair geom1=\"foot2_left_{player_type}\" geom2=\"floor\"/>\n    <pair geom1=\"foot2_right_{player_type}\" geom2=\"floor\"/>\n    <pair geom1=\"foot1_left_{player_type}\" geom2=\"ball\"/>\n    <pair geom1=\"foot2_left_{player_type}\" geom2=\"ball\"/>\n    {extra_contacts}\n    \"\"\"\n    return contact_xml\n\n\ndef get_actuators(player_type: str) -> str:\n    \"\"\"Returns the actuators XML.\n\n    Parameters:\n        player_type (str): can be 'striker' or 'keeper'.\n\n    Returns:\n        str: The actuators XML to generate a MJX model.\n    \"\"\"\n    actuator_xml = f\"\"\"\n    <motor name=\"abdomen_y_{player_type}\"       gear=\"40\"  joint=\"abdomen_y_{player_type}\"/>\n    <motor name=\"abdomen_z_{player_type}\"       gear=\"40\"  joint=\"abdomen_z_{player_type}\"/>\n    <motor name=\"abdomen_x_{player_type}\"       gear=\"40\"  joint=\"abdomen_x_{player_type}\"/>\n    <motor name=\"hip_x_right_{player_type}\"     gear=\"40\"  joint=\"hip_x_right_{player_type}\"/>\n    <motor name=\"hip_z_right_{player_type}\"     gear=\"40\"  joint=\"hip_z_right_{player_type}\"/>\n    <motor name=\"hip_y_right_{player_type}\"     gear=\"120\" joint=\"hip_y_right_{player_type}\"/>\n    <motor name=\"knee_right_{player_type}\"      gear=\"80\"  joint=\"knee_right_{player_type}\"/>\n    <motor name=\"ankle_x_right_{player_type}\"   gear=\"20\"  joint=\"ankle_x_right_{player_type}\"/>\n    <motor name=\"ankle_y_right_{player_type}\"   gear=\"20\"  joint=\"ankle_y_right_{player_type}\"/>\n    <motor name=\"hip_x_left_{player_type}\"      gear=\"40\"  joint=\"hip_x_left_{player_type}\"/>\n    <motor name=\"hip_z_left_{player_type}\"      gear=\"40\"  joint=\"hip_z_left_{player_type}\"/>\n    <motor name=\"hip_y_left_{player_type}\"      gear=\"120\" joint=\"hip_y_left_{player_type}\"/>\n    <motor name=\"knee_left_{player_type}\"       gear=\"80\"  joint=\"knee_left_{player_type}\"/>\n    <motor name=\"ankle_x_left_{player_type}\"    gear=\"20\"  joint=\"ankle_x_left_{player_type}\"/>\n    <motor name=\"ankle_y_left_{player_type}\"    gear=\"20\"  joint=\"ankle_y_left_{player_type}\"/>\n    <motor name=\"shoulder1_right_{player_type}\" gear=\"20\"  joint=\"shoulder1_right_{player_type}\"/>\n    <motor name=\"shoulder2_right_{player_type}\" gear=\"20\"  joint=\"shoulder2_right_{player_type}\"/>\n    <motor name=\"elbow_right_{player_type}\"     gear=\"40\"  joint=\"elbow_right_{player_type}\"/>\n    <motor name=\"shoulder1_left_{player_type}\"  gear=\"20\"  joint=\"shoulder1_left_{player_type}\"/>\n    <motor name=\"shoulder2_left_{player_type}\"  gear=\"20\"  joint=\"shoulder2_left_{player_type}\"/>\n    <motor name=\"elbow_left_{player_type}\"      gear=\"40\"  joint=\"elbow_left_{player_type}\"/>\n    \"\"\"\n    return actuator_xml\n\n\nball_material = \"\"\"\n    <texture name=\"texgeom\" type=\"cube\" builtin=\"flat\" mark=\"cross\" width=\"128\" height=\"128\" \n        rgb1=\"0.6 0.6 0.6\" rgb2=\"0.6 0.6 0.6\" markrgb=\"1 1 1\"/>\n    <material name=\"ball\" texture=\"texgeom\" texuniform=\"true\" rgba=\".1 .9 .1 1\" />\n    \"\"\"\nball_default = f\"\"\"\n    <default class=\"ball\">\n      <geom type=\"sphere\" material=\"ball\" size=\"{ball_size}\" mass=\"0.045\" friction=\"0.7 0.075 0.075\" solref=\"0.02 1.0\"/>\n    </default>\n    \"\"\"\nball_body = f\"\"\"\n    <body name=\"ball\" pos=\"{ball_x} 0 {ball_size}\" quat=\"0.632456 -0.632456 0.316228 0.316228\">\n      <freejoint/>\n      <geom class=\"ball\" name=\"ball\"/>\n    </body>\n    \"\"\"\ngoal_body = f\"\"\"\n    <body name=\"post1\" pos=\"{goal_distance} {post1_y} 0\">\n        <geom name=\"post_geom1\" type=\"box\" size=\"0.05 0.05 {goal_height}\" rgba=\"1 1 1 1\"/>\n    </body>\n    <body name=\"post2\" pos=\"{goal_distance} {post2_y} 0\">\n        <geom name=\"post_geom2\" type=\"box\" size=\"0.05 0.05 {goal_height}\" rgba=\"1 1 1 1\"/>\n    </body>\n\n    <body name=\"bar\" pos=\"{goal_distance} 0 {goal_height}\">\n        <geom name=\"bar_geom\" type=\"box\" size=\"0.05 {goal_width / 2} 0.05\" rgba=\"1 1 1 1\"/>\n    </body>\n\"\"\"\n    \nxml = f\"\"\"\n<mujoco model=\"Humanoid and a ball\">\n  <option timestep=\"{mj_model_timestep}\" iterations=\"1\" ls_iterations=\"4\">\n    <flag eulerdamp=\"disable\"/>\n  </option>\n\n  <visual>\n    <map force=\"0.1\" zfar=\"30\" znear=\"0.1\" />\n    <rgba haze=\"0.15 0.25 0.35 1\"/>\n    <global offwidth=\"2560\" offheight=\"1440\" elevation=\"-20\" azimuth=\"120\"/>\n    <quality shadowsize=\"4096\" offsamples=\"8\"/>\n  </visual>\n\n  <statistic center=\"0 0 0.7\" extent=\"4\"/>\n\n  <asset>\n    <texture type=\"skybox\" builtin=\"gradient\" rgb1=\".3 .5 .7\" rgb2=\"0 0 0\" width=\"32\" height=\"512\"/>\n    <texture name=\"body\" type=\"cube\" builtin=\"flat\" mark=\"cross\" width=\"128\" height=\"128\" rgb1=\"0.8 0.6 0.4\" rgb2=\"0.8 0.6 0.4\" markrgb=\"1 1 1\" random=\"0.01\"/>\n    <material name=\"body\" texture=\"body\" texuniform=\"true\" rgba=\"0.8 0.6 .4 1\"/>\n\n    <texture type=\"skybox\" builtin=\"gradient\" width=\"512\" height=\"512\" rgb1=\".4 .6 .8\" rgb2=\"0 0 0\"/>\n    <texture name=\"texplane\" type=\"2d\" builtin=\"checker\" rgb1=\".4 .4 .4\" rgb2=\".6 .6 .6\"\n             width=\"512\" height=\"512\"/>\n    <material name='MatPlane' reflectance='0.3' texture=\"texplane\" texrepeat=\"1 1\" texuniform=\"true\"\n              rgba=\".7 .7 .7 1\"/>\n    {ball_material}\n  </asset>\n\n  <default>\n    {ball_default}\n    <motor ctrlrange=\"-1 1\" ctrllimited=\"true\"/>\n    <default class=\"body\">\n\n      <!-- geoms -->\n      <geom type=\"capsule\" condim=\"3\" friction=\".7\" solimp=\".9 .99 .003\" solref=\".015 1\" material=\"body\" contype=\"0\" conaffinity=\"0\"/>\n      <default class=\"thigh\">\n        <geom size=\".06\"/>\n      </default>\n      <default class=\"shin\">\n        <geom fromto=\"0 0 0 0 0 -.3\"  size=\".049\"/>\n      </default>\n      <default class=\"foot\">\n        <geom size=\".027\"/>\n        <default class=\"foot1_striker\">\n          <geom fromto=\"-.07 -.01 0 .14 -.03 0\"/>\n        </default>\n        <default class=\"foot2_striker\">\n          <geom fromto=\"-.07 .01 0 .14  .03 0\"/>\n        </default>\n        <default class=\"foot1_keeper\">\n          <geom fromto=\"-.14 -.03 0 .07 -.01 0\"/>\n        </default>\n        <default class=\"foot2_keeper\">\n          <geom fromto=\"-.14 .03 0 .07  .01 0\"/>\n        </default>\n      </default>\n      <default class=\"arm_upper\">\n        <geom size=\".04\"/>\n      </default>\n      <default class=\"arm_lower\">\n        <geom size=\".031\"/>\n      </default>\n      <default class=\"hand\">\n        <geom type=\"sphere\" size=\".04\"/>\n      </default>\n\n      <!-- joints -->\n      <joint type=\"hinge\" damping=\".2\" stiffness=\"1\" armature=\".01\" limited=\"true\" solimplimit=\"0 .99 .01\"/>\n      <default class=\"joint_big\">\n        <joint damping=\"5\" stiffness=\"10\"/>\n        <default class=\"hip_x\">\n          <joint range=\"-30 10\"/>\n        </default>\n        <default class=\"hip_z\">\n          <joint range=\"-60 35\"/>\n        </default>\n        <default class=\"hip_y\">\n          <joint range=\"-150 20\"/>\n        </default>\n        <default class=\"joint_big_stiff\">\n          <joint stiffness=\"20\"/>\n        </default>\n      </default>\n      <default class=\"knee\">\n        <joint pos=\"0 0 .02\" range=\"-160 2\"/>\n      </default>\n      <default class=\"ankle\">\n        <joint range=\"-50 50\"/>\n        <default class=\"ankle_y\">\n          <joint pos=\"0 0 .08\" stiffness=\"6\"/>\n        </default>\n        <default class=\"ankle_x\">\n          <joint pos=\"0 0 .04\" stiffness=\"3\"/>\n        </default>\n      </default>\n      <default class=\"shoulder\">\n        <joint range=\"-85 60\"/>\n      </default>\n      <default class=\"elbow\">\n        <joint range=\"-100 50\" stiffness=\"0\"/>\n      </default>\n    </default>\n  </default>\n\n  <worldbody>\n    <light directional=\"true\" diffuse=\".8 .8 .8\" pos=\"0 0 10\" dir=\"0 0 -10\"/>\n    <geom name=\"floor\" type=\"plane\" size=\"{goal_distance * 2} 5 .05\" material=\"MatPlane\" condim=\"3\"/>\n\n    {get_humanoid(\"striker\")}\n    {get_humanoid(\"keeper\")}\n\n    {goal_body}\n    {ball_body}\n  </worldbody>\n\n  <contact>\n    {get_contacts(\"striker\")}\n    {get_contacts(\"keeper\")}\n    <pair geom1=\"post_geom1\" geom2=\"ball\"/>\n    <pair geom1=\"post_geom2\" geom2=\"ball\"/>\n    <pair geom1=\"bar_geom\" geom2=\"ball\"/>\n  </contact>\n\n  <actuator>\n    {get_actuators(\"striker\")}\n    {get_actuators(\"keeper\")}\n  </actuator>\n</mujoco>\n\"\"\"","metadata":{"id":"G5RKmMLkUuC8","cellView":"form","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Humanoid Env\n\nclass Humanoid(PipelineEnv):\n\n  def __init__(\n      self,\n      terminate_when_unhealthy=True,\n      reset_noise_scale=1e-2,\n      exclude_current_positions_from_observation=True,\n      **kwargs,\n  ):\n    mj_model = mujoco.MjModel.from_xml_string(xml)\n    mj_model.opt.solver = mujoco.mjtSolver.mjSOL_CG\n    mj_model.opt.iterations = 6\n    mj_model.opt.ls_iterations = 6\n\n    sys = mjcf.load_model(mj_model)\n\n    physics_steps_per_control_step = 5\n    kwargs['n_frames'] = kwargs.get(\n        'n_frames', physics_steps_per_control_step)\n    kwargs['backend'] = 'mjx'\n\n    super().__init__(sys, **kwargs)\n\n    self._terminate_when_unhealthy = terminate_when_unhealthy\n    self._reset_noise_scale = reset_noise_scale\n    self._exclude_current_positions_from_observation = (\n        exclude_current_positions_from_observation\n    )\n\n  def reset(self, rng: jp.ndarray) -> State:\n    \"\"\"Resets the environment to an initial state.\"\"\"\n    rng, rng1, rng2 = jax.random.split(rng, 3)\n\n    low, hi = -self._reset_noise_scale, self._reset_noise_scale\n    qpos = self.sys.qpos0 + jax.random.uniform(\n        rng1, (self.sys.nq,), minval=low, maxval=hi\n    )\n    qvel = jax.random.uniform(\n        rng2, (self.sys.nv,), minval=low, maxval=hi\n    )\n    \n    data = self.pipeline_init(qpos, qvel)\n\n    obs = self._get_obs(data, jp.zeros(self.sys.nu))\n    reward, done, zero = jp.zeros(3)\n    metrics = {\n        \"reward\": zero,\n        \"goal\": zero,\n    }\n    return State(data, obs, reward, done, metrics)\n\n  def step(self, state: State, action: jp.ndarray) -> State:\n    \"\"\"Runs one timestep of the environment's dynamics.\"\"\"\n    data0 = state.pipeline_state\n    data = self.pipeline_step(data0, action)\n    \n    if training_agent == \"striker\":\n        reward, done = self._get_striker_reward(state, action, data0, data)\n    elif training_agent == \"keeper\":\n        reward, done = self._get_keeper_reward(state, action, data0, data)\n    else:\n        raise Exception(f\"{training_agent=} isn't supported.\")\n    \n    obs = self._get_obs(data, action)\n    return state.replace(\n        pipeline_state=data, obs=obs, reward=reward, done=done\n    )\n\n  def _get_obs(\n      self, data: mjx.Data, action: jp.ndarray\n  ) -> jp.ndarray:\n    \"\"\"Observes humanoid body and ball position, velocities, and angles.\"\"\"\n    position = data.qpos\n    if self._exclude_current_positions_from_observation:\n      position = position[2:]\n\n    # external_contact_forces are excluded\n    return jp.concatenate([\n        # qpos: position / nq: number of generalized coordinates = dim(qpos)\n        position,\n        # qvel: velocity / nv: number of degrees of freedom = dim(qvel)\n        data.qvel,\n        # cinert: com-based body inertia and mass / (nbody, 10)\n        data.cinert[1:].ravel(),\n        # cvel: com-based velocity [3D rot; 3D tran] / (nbody, 6)\n        data.cvel[1:].ravel(),\n        # qfrc_actuator: actuator force / nv: number of degrees of freedom\n        data.qfrc_actuator,\n    ])\n\n\n  def _get_striker_reward(\n      self, state: State,  action: jp.ndarray, data0: mjx.Data, data: mjx.Data\n  ) -> Tuple[jp.ndarray, jp.ndarray]:\n    \"\"\"Apply reward func for striker.\n    \n    Based on distance to goal, ball speed and whether it's a goal\n    \"\"\"\n    ctrl_cost_weight = 0.1\n    healthy_reward = 1.0\n    healthy_z_range = (1.0, 3.0)\n    ball_reward = 5.0\n    ball_healthy_z_range = (0.0, goal_height)\n    ball_reward_target_x = goal_distance\n    goal_reward = 1000\n    \n    com_before_ball = data0.subtree_com[-1]\n    com_after_ball = data.subtree_com[-1]\n    \n    min_z, max_z = healthy_z_range\n    is_healthy = jp.where(data.q[torso_index] < min_z, 0.0, 1.0)\n    is_healthy = jp.where(data.q[torso_index] > max_z, 0.0, is_healthy)\n\n    ball_min_z, ball_max_z = ball_healthy_z_range\n    is_healthy = jp.where(com_after_ball[2] < ball_min_z, 0.0, is_healthy)\n    is_healthy = jp.where(com_after_ball[2] > ball_max_z, 0.0, is_healthy)\n    \n    is_healthy = jp.where(com_after_ball[0] > goal_distance * 1.5, 0.0, is_healthy)\n    \n    ctrl_cost = ctrl_cost_weight * jp.sum(jp.square(action))\n    \n    velocity = (com_after_ball - com_before_ball) / self.dt\n    distance_goal = jp.sqrt(jp.square(ball_reward_target_x - com_after_ball[0]))\n    \n    ball_velocity_reward = jp.where(com_after_ball[0] > goal_distance, 0.0, ball_reward * velocity[0])\n    ball_reward = jp.where(com_after_ball[0] > goal_distance, 0.0, ball_reward * (1 - (distance_goal / ball_reward_target_x)))\n    \n    is_goal = self._is_goal(com_before_ball, com_after_ball)\n    goal_reward = goal_reward * is_goal\n    \n    reward = ball_reward + ball_velocity_reward + healthy_reward - ctrl_cost + goal_reward\n\n    state.metrics.update(\n        reward=reward,\n        goal=is_goal,\n    )\n    \n    done = 1.0 - is_healthy\n    return reward, done\n\n\n  def _get_keeper_reward(\n      self, state: State,  action: jp.ndarray, data0: mjx.Data, data: mjx.Data\n  ) -> Tuple[jp.ndarray, jp.ndarray]:\n    \"\"\"Apply reward func for keeper.\n    \n    Based on distance to y_z ball coordinates, and whether it's a goal\n    \"\"\"\n    ctrl_cost_weight = 0.1\n    healthy_reward = 1.0\n    healthy_z_range = (1.0, 3.0)\n    distance_ball_reward = 5.0\n    distance_ball_target = 0.0\n    goal_reward = -1000\n    \n    com_before_ball = data0.subtree_com[-1]\n    com_after_ball = data.subtree_com[-1]\n    com_after_left_hand = data.subtree_com[32]\n    com_after_right_hand = data.subtree_com[29]\n    distance_left_hand = jp.sqrt(jp.square(com_after_ball[1] - com_after_left_hand[1]) + jp.square(com_after_ball[2] - com_after_left_hand[2]))\n    distance_right_hand = jp.sqrt(jp.square(com_after_ball[1] - com_after_right_hand[1]) + jp.square(com_after_ball[2] - com_after_right_hand[2]))\n    distance_hands = jp.mean(jp.array([distance_left_hand, distance_right_hand]))\n    \n    min_z, max_z = healthy_z_range\n    is_healthy = jp.where(data.q[torso_index] < min_z, 0.0, 1.0)\n    is_healthy = jp.where(data.q[torso_index] > max_z, 0.0, is_healthy)\n    is_healthy = jp.where(com_after_ball[0] > goal_distance * 1.5, 0.0, is_healthy)\n    \n    ctrl_cost = ctrl_cost_weight * jp.sum(jp.square(action))\n    \n    distance_ball_reward = distance_ball_reward * (1 - (distance_hands / goal_width))\n    \n    is_goal = self._is_goal(com_before_ball, com_after_ball)\n    goal_reward = goal_reward * is_goal\n    \n    reward = distance_ball_reward - ctrl_cost + goal_reward\n\n    state.metrics.update(\n        reward=reward,\n        goal=is_goal,\n    )\n    \n    done = 1.0 - is_healthy\n    return reward, done\n  \n    \n  def _is_goal(\n      self, com_before_ball: jp.ndarray, com_after_ball: jp.ndarray\n  ) -> jp.ndarray:\n    \"\"\"Check if it's a goal.\"\"\"\n    is_goal = jp.where(com_before_ball[0] < goal_distance, 1.0, 0.0)\n    is_goal = jp.where(com_after_ball[0] >= goal_distance, is_goal, 0.0)\n    is_goal = jp.where(com_after_ball[1] > post1_y, is_goal, 0.0)\n    is_goal = jp.where(com_after_ball[1] < post2_y, is_goal, 0.0)\n    is_goal = jp.where(com_after_ball[2] < goal_height, is_goal, 0.0)\n    return is_goal\n    \n\nenvs.register_environment(\"humanoid\", Humanoid)","metadata":{"id":"mtGMYNLE3QJN","cellView":"form","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5 - Visualize a rollout\n\nLet's instantiate the environment and visualize a short rollout.","metadata":{"id":"P1K6IznI2y83"}},{"cell_type":"code","source":"# instantiate the environment\nenv_name = \"humanoid\"\nenv = envs.get_environment(env_name)\n\n# define the jit reset/step functions\njit_reset = jax.jit(env.reset)\njit_step = jax.jit(env.step)","metadata":{"id":"EhKLFK54C1CH","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initialize the state\nstate = jit_reset(jax.random.PRNGKey(0))\nprint(f\"Observations size: {len(state.obs)}\")\nprint(f\"Actions size: {env.sys.nu}\")\n\nrollout = [state.pipeline_state]\n\n# grab a trajectory\nfor i in range(50):\n  # ctrl: control / nu: number of actuators/controls = dim(ctrl)\n  ctrl = -0.1 * jp.ones(env.sys.nu)\n  state = jit_step(state, ctrl)\n  rollout.append(state.pipeline_state)\n\nmedia.show_video(env.render(rollout, camera=\"back_striker\", height=480, width=640), fps=1.0 / env.dt)","metadata":{"id":"Ph8u-v2Q2xLS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6 - Define the training functions\n\nLet's define the training functions using [PPO](https://openai.com/research/openai-baselines-ppo) to make the Humanoids take and stop penalties.","metadata":{"id":"BQDG6NQ1CbZD"}},{"cell_type":"code","source":"# Define the Acting/Evaluator (adapted from https://github.com/google/brax)\n\n\"\"\"Brax training acting functions.\"\"\"\n\nimport time\nfrom typing import Callable, Sequence, Tuple, Union\n\nfrom brax import envs\nfrom brax.training.types import Metrics\nfrom brax.training.types import Policy\nfrom brax.training.types import PolicyParams\nfrom brax.training.types import PRNGKey\nfrom brax.training.types import Transition\nfrom brax.v1 import envs as envs_v1\nimport jax\nimport jax.numpy as jp\nimport numpy as np\n\nActingState = Union[envs.State, envs_v1.State]\nActingEnv = Union[envs.Env, envs_v1.Env, envs_v1.Wrapper]\n\n\ndef actor_step(\n    env: ActingEnv,\n    env_state: ActingState,\n    training_policy: Policy,\n    non_training_policy: Policy,\n    key: PRNGKey,\n    extra_fields: Sequence[str] = ()\n) -> Tuple[ActingState, Transition]:\n  \"\"\"Collect data.\"\"\"\n  # We care about ['policy_extras']['raw_action'] and ['policy_extras']['log_prob'] \n  # for computing PPO loss, so the relevant policy_extras come from the training_policy\n  training_agent_actions, policy_extras = training_policy(env_state.obs, key)\n  non_training_agent_actions, _ = non_training_policy(env_state.obs, key)\n  # This is prone to error as the way to concatenate the actions \n  # depends on the order of the actuators in the xml\n  if training_agent == \"striker\":\n    actions = jp.concatenate((training_agent_actions, non_training_agent_actions), axis=1)\n  elif training_agent == \"keeper\":\n    actions = jp.concatenate((non_training_agent_actions, training_agent_actions), axis=1)\n  else:\n    raise Exception(f\"{training_agent=} isn't supported.\") \n  \n  assert actions.shape[1] == env.action_size\n  nstate = env.step(env_state, actions)\n  state_extras = {x: nstate.info[x] for x in extra_fields}\n  return nstate, Transition(  # pytype: disable=wrong-arg-types  # jax-ndarray\n      observation=env_state.obs,\n      action=actions,\n      reward=nstate.reward,\n      discount=1 - nstate.done,\n      next_observation=nstate.obs,\n      extras={\n          'policy_extras': policy_extras,\n          'state_extras': state_extras\n      })\n\n\ndef generate_unroll(\n    env: ActingEnv,\n    env_state: ActingState,\n    training_policy: Policy,\n    non_training_policy: Policy,\n    key: PRNGKey,\n    unroll_length: int,\n    extra_fields: Sequence[str] = ()\n) -> Tuple[ActingState, Transition]:\n  \"\"\"Collect trajectories of given unroll_length.\"\"\"\n\n  @jax.jit\n  def f(carry, unused_t):\n    state, current_key = carry\n    current_key, next_key = jax.random.split(current_key)\n    nstate, transition = actor_step(\n        env, state, training_policy, non_training_policy, current_key, extra_fields=extra_fields)\n    return (nstate, next_key), transition\n\n  (final_state, _), data = jax.lax.scan(\n      f, (env_state, key), (), length=unroll_length)\n  return final_state, data\n\n\n# TODO: Consider moving this to its own file.\nclass Evaluator:\n  \"\"\"Class to run evaluations.\"\"\"\n\n  def __init__(self, eval_env: envs.Env,\n               eval_policy_fn: Callable[[PolicyParams],\n                                        Policy], num_eval_envs: int,\n               episode_length: int, action_repeat: int, key: PRNGKey):\n    \"\"\"Init.\n\n    Args:\n      eval_env: Batched environment to run evals on.\n      eval_policy_fn: Function returning the policy from the policy parameters.\n      num_eval_envs: Each env will run 1 episode in parallel for each eval.\n      episode_length: Maximum length of an episode.\n      action_repeat: Number of physics steps per env step.\n      key: RNG key.\n    \"\"\"\n    self._key = key\n    self._eval_walltime = 0.\n\n    eval_env = envs.training.EvalWrapper(eval_env)\n\n    def generate_eval_unroll(training_policy_params: PolicyParams,\n                             non_training_policy_params: PolicyParams,\n                             key: PRNGKey) -> ActingState:\n      reset_keys = jax.random.split(key, num_eval_envs)\n      eval_first_state = eval_env.reset(reset_keys)\n      return generate_unroll(\n          eval_env,\n          eval_first_state,\n          eval_policy_fn(training_policy_params),\n          eval_policy_fn(non_training_policy_params),\n          key,\n          unroll_length=episode_length // action_repeat)[0]\n\n    self._generate_eval_unroll = jax.jit(generate_eval_unroll)\n    self._steps_per_unroll = episode_length * num_eval_envs\n\n  def run_evaluation(self,\n                     training_policy_params: PolicyParams,\n                     non_training_policy_params: PolicyParams,\n                     training_metrics: Metrics,\n                     aggregate_episodes: bool = True) -> Metrics:\n    \"\"\"Run one epoch of evaluation.\"\"\"\n    self._key, unroll_key = jax.random.split(self._key)\n\n    t = time.time()\n    eval_state = self._generate_eval_unroll(training_policy_params,\n                                            non_training_policy_params,\n                                            unroll_key)\n    eval_metrics = eval_state.info['eval_metrics']\n    eval_metrics.active_episodes.block_until_ready()\n    epoch_eval_time = time.time() - t\n    metrics = {}\n    for fn in [np.mean, np.std, np.max]:\n      suffix = '_std' if fn == np.std else '_max' if fn == np.max else ''\n      metrics.update(\n          {\n              f'eval/episode_{name}{suffix}': (\n                  fn(value) if aggregate_episodes else value\n              )\n              for name, value in eval_metrics.episode_metrics.items()\n          }\n      )\n    metrics['eval/avg_episode_length'] = np.mean(eval_metrics.episode_steps)\n    metrics['eval/epoch_eval_time'] = epoch_eval_time\n    metrics['eval/sps'] = self._steps_per_unroll / epoch_eval_time\n    self._eval_walltime = self._eval_walltime + epoch_eval_time\n    metrics = {\n        'eval/walltime': self._eval_walltime,\n        **training_metrics,\n        **metrics\n    }\n\n    return metrics  # pytype: disable=bad-return-type  # jax-ndarray","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the Training Function (adapted from https://github.com/google/brax)\n# This module was changed to allow training two agents in the same environment\n\n\"\"\"Proximal policy optimization training.\n\nSee: https://arxiv.org/pdf/1707.06347.pdf\n\"\"\"\n\nimport functools\nimport time\nfrom typing import Callable, Optional, Tuple, Union\n\nfrom absl import logging\nfrom brax import base\nfrom brax import envs\nfrom brax.training import gradients\nfrom brax.training import pmap\nfrom brax.training import types\nfrom brax.training.acme import running_statistics\nfrom brax.training.acme import specs\nfrom brax.training.agents.ppo import losses as ppo_losses\nfrom brax.training.agents.ppo import networks as ppo_networks\nfrom brax.training.types import Params, PolicyParams, PreprocessorParams\nfrom brax.training.types import PRNGKey\nfrom brax.v1 import envs as envs_v1\nimport flax\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport optax\n\n\nInferenceParams = Tuple[running_statistics.NestedMeanStd, Params]\nMetrics = types.Metrics\nValueParams = Any\n\n_PMAP_AXIS_NAME = 'i'\n\n\n@flax.struct.dataclass\nclass TrainingState:\n  \"\"\"Contains training state for the learner.\"\"\"\n  optimizer_state: optax.OptState\n  params: ppo_losses.PPONetworkParams\n  normalizer_params: running_statistics.RunningStatisticsState\n  env_steps: jnp.ndarray\n\n\ndef _unpmap(v):\n  return jax.tree_util.tree_map(lambda x: x[0], v)\n\n\ndef _strip_weak_type(tree):\n  # brax user code is sometimes ambiguous about weak_type.  in order to\n  # avoid extra jit recompilations we strip all weak types from user input\n  def f(leaf):\n    leaf = jnp.asarray(leaf)\n    return leaf.astype(leaf.dtype)\n  return jax.tree_util.tree_map(f, tree)\n\n\ndef train(\n    environment: Union[envs_v1.Env, envs.Env],\n    num_timesteps: int,\n    episode_length: int,\n    action_repeat: int = 1,\n    num_envs: int = 1,\n    max_devices_per_host: Optional[int] = None,\n    num_eval_envs: int = 128,\n    learning_rate: float = 1e-4,\n    entropy_cost: float = 1e-4,\n    discounting: float = 0.9,\n    seed: int = 0,\n    unroll_length: int = 10,\n    batch_size: int = 32,\n    num_minibatches: int = 16,\n    num_updates_per_batch: int = 2,\n    num_evals: int = 1,\n    num_resets_per_eval: int = 0,\n    normalize_observations: bool = False,\n    reward_scaling: float = 1.0,\n    clipping_epsilon: float = 0.3,\n    gae_lambda: float = 0.95,\n    deterministic_eval: bool = False,\n    network_factory: types.NetworkFactory[\n        ppo_networks.PPONetworks\n    ] = ppo_networks.make_ppo_networks,\n    progress_fn: Callable[[int, Metrics], None] = lambda *args: None,\n    normalize_advantage: bool = True,\n    eval_env: Optional[envs.Env] = None,\n    policy_params_fn: Callable[..., None] = lambda *args: None,\n    randomization_fn: Optional[\n        Callable[[base.System, jnp.ndarray], Tuple[base.System, base.System]]\n    ] = None,\n    striker_params: Optional[\n        Tuple[PreprocessorParams, PolicyParams, ValueParams]\n    ] = None,\n    keeper_params: Optional[\n        Tuple[PreprocessorParams, PolicyParams, ValueParams]\n    ] = None,\n    training_agent: str = \"striker\",\n    humanoid_actuators: int = 21,\n):\n  \"\"\"PPO training.\n\n  Args:\n    environment: the environment to train\n    num_timesteps: the total number of environment steps to use during training\n    episode_length: the length of an environment episode\n    action_repeat: the number of timesteps to repeat an action\n    num_envs: the number of parallel environments to use for rollouts\n      NOTE: `num_envs` must be divisible by the total number of chips since each\n        chip gets `num_envs // total_number_of_chips` environments to roll out\n      NOTE: `batch_size * num_minibatches` must be divisible by `num_envs` since\n        data generated by `num_envs` parallel envs gets used for gradient\n        updates over `num_minibatches` of data, where each minibatch has a\n        leading dimension of `batch_size`\n    max_devices_per_host: maximum number of chips to use per host process\n    num_eval_envs: the number of envs to use for evaluation. Each env will run 1\n      episode, and all envs run in parallel during eval.\n    learning_rate: learning rate for ppo loss\n    entropy_cost: entropy reward for ppo loss, higher values increase entropy\n      of the policy\n    discounting: discounting rate\n    seed: random seed\n    unroll_length: the number of timesteps to unroll in each environment. The\n      PPO loss is computed over `unroll_length` timesteps\n    batch_size: the batch size for each minibatch SGD step\n    num_minibatches: the number of times to run the SGD step, each with a\n      different minibatch with leading dimension of `batch_size`\n    num_updates_per_batch: the number of times to run the gradient update over\n      all minibatches before doing a new environment rollout\n    num_evals: the number of evals to run during the entire training run.\n      Increasing the number of evals increases total training time\n    num_resets_per_eval: the number of environment resets to run between each\n      eval. The environment resets occur on the host\n    normalize_observations: whether to normalize observations\n    reward_scaling: float scaling for reward\n    clipping_epsilon: clipping epsilon for PPO loss\n    gae_lambda: General advantage estimation lambda\n    deterministic_eval: whether to run the eval with a deterministic policy\n    network_factory: function that generates networks for policy and value\n      functions\n    progress_fn: a user-defined callback function for reporting/plotting metrics\n    normalize_advantage: whether to normalize advantage estimate\n    eval_env: an optional environment for eval only, defaults to `environment`\n    policy_params_fn: a user-defined callback function that can be used for\n      saving policy checkpoints\n    randomization_fn: a user-defined callback function that generates randomized\n      environments\n    striker_params: striker params; includes normalizer_params\n      and policy and value network params\n    keeper_params: keeper params; includes normalizer_params\n      and policy and value network params\n    training_agent: agent to train; can be 'striker' or 'keeper'\n    humanoid_actuators: num of humanoid actuators; both for 'striker' and 'keeper'\n\n  Returns:\n    Tuple of (make_policy function, network params, metrics, max score network params)\n  \"\"\"\n  assert batch_size * num_minibatches % num_envs == 0\n  xt = time.time()\n\n  process_count = jax.process_count()\n  process_id = jax.process_index()\n  local_device_count = jax.local_device_count()\n  local_devices_to_use = local_device_count\n  if max_devices_per_host:\n    local_devices_to_use = min(local_devices_to_use, max_devices_per_host)\n  logging.info(\n      'Device count: %d, process count: %d (id %d), local device count: %d, '\n      'devices to be used count: %d', jax.device_count(), process_count,\n      process_id, local_device_count, local_devices_to_use)\n  device_count = local_devices_to_use * process_count\n\n  # The number of environment steps executed for every training step.\n  env_step_per_training_step = (\n      batch_size * unroll_length * num_minibatches * action_repeat)\n  num_evals_after_init = max(num_evals - 1, 1)\n  # The number of training_step calls per training_epoch call.\n  # equals to ceil(num_timesteps / (num_evals * env_step_per_training_step *\n  #                                 num_resets_per_eval))\n  num_training_steps_per_epoch = np.ceil(\n      num_timesteps\n      / (\n          num_evals_after_init\n          * env_step_per_training_step\n          * max(num_resets_per_eval, 1)\n      )\n  ).astype(int)\n\n  key = jax.random.PRNGKey(seed)\n  global_key, local_key = jax.random.split(key)\n  del key\n  local_key = jax.random.fold_in(local_key, process_id)\n  local_key, key_env, eval_key = jax.random.split(local_key, 3)\n  # key_networks should be global, so that networks are initialized the same\n  # way for different processes.\n  key_policy, key_value = jax.random.split(global_key)\n  key_training_policy, key_non_training_policy = jax.random.split(key_policy)\n  del global_key\n  del key_policy\n\n  assert num_envs % device_count == 0\n\n  v_randomization_fn = None\n  if randomization_fn is not None:\n    randomization_batch_size = num_envs // local_device_count\n    # all devices gets the same randomization rng\n    randomization_rng = jax.random.split(key_env, randomization_batch_size)\n    v_randomization_fn = functools.partial(\n        randomization_fn, rng=randomization_rng\n    )\n\n  if isinstance(environment, envs.Env):\n    wrap_for_training = envs.training.wrap\n  else:\n    wrap_for_training = envs_v1.wrappers.wrap_for_training\n\n  env = wrap_for_training(\n      environment,\n      episode_length=episode_length,\n      action_repeat=action_repeat,\n      randomization_fn=v_randomization_fn,\n  )\n\n  reset_fn = jax.jit(jax.vmap(env.reset))\n  key_envs = jax.random.split(key_env, num_envs // process_count)\n  key_envs = jnp.reshape(key_envs,\n                         (local_devices_to_use, -1) + key_envs.shape[1:])\n  env_state = reset_fn(key_envs)\n\n  normalize = lambda x, y: x\n  if normalize_observations:\n    normalize = running_statistics.normalize\n  \n  assert humanoid_actuators * 2 == env.action_size\n  ppo_network = network_factory(\n      env_state.obs.shape[-1],\n      humanoid_actuators,\n      preprocess_observations_fn=normalize)\n  make_policy = ppo_networks.make_inference_fn(ppo_network)\n\n  optimizer = optax.adam(learning_rate=learning_rate)\n\n  loss_fn = functools.partial(\n      ppo_losses.compute_ppo_loss,\n      ppo_network=ppo_network,\n      entropy_cost=entropy_cost,\n      discounting=discounting,\n      reward_scaling=reward_scaling,\n      gae_lambda=gae_lambda,\n      clipping_epsilon=clipping_epsilon,\n      normalize_advantage=normalize_advantage)\n\n  gradient_update_fn = gradients.gradient_update_fn(\n      loss_fn, optimizer, pmap_axis_name=_PMAP_AXIS_NAME, has_aux=True)\n\n  if training_agent == \"striker\":\n    training_params = striker_params\n    non_training_params = keeper_params\n  elif training_agent == \"keeper\":\n    training_params = keeper_params\n    non_training_params = striker_params\n  else:\n    raise Exception(f\"{training_agent=} isn't supported.\") \n  \n  if training_params is None:\n    init_params = ppo_losses.PPONetworkParams(\n      policy=ppo_network.policy_network.init(key_training_policy),\n      value=ppo_network.value_network.init(key_value))\n    normalizer_params = running_statistics.init_state(\n      specs.Array(env_state.obs.shape[-1:], jnp.dtype('float32')))\n  else:\n    init_params = ppo_losses.PPONetworkParams(\n      policy=training_params[1],\n      value=training_params[2])\n    normalizer_params = training_params[0]\n\n  if non_training_params is None:\n    non_training_params = []\n    non_training_params.append(running_statistics.init_state(\n      specs.Array(env_state.obs.shape[-1:], jnp.dtype('float32'))))\n    non_training_params.append(ppo_network.policy_network.init(key_non_training_policy))\n\n    \n  def minibatch_step(\n      carry, data: types.Transition,\n      normalizer_params: running_statistics.RunningStatisticsState):\n    optimizer_state, params, key = carry\n    key, key_loss = jax.random.split(key)\n    (_, metrics), params, optimizer_state = gradient_update_fn(\n        params,\n        normalizer_params,\n        data,\n        key_loss,\n        optimizer_state=optimizer_state)\n\n    return (optimizer_state, params, key), metrics\n\n  def sgd_step(carry, unused_t, data: types.Transition,\n               normalizer_params: running_statistics.RunningStatisticsState):\n    optimizer_state, params, key = carry\n    key, key_perm, key_grad = jax.random.split(key, 3)\n\n    def convert_data(x: jnp.ndarray):\n      x = jax.random.permutation(key_perm, x)\n      x = jnp.reshape(x, (num_minibatches, -1) + x.shape[1:])\n      return x\n\n    shuffled_data = jax.tree_util.tree_map(convert_data, data)\n    (optimizer_state, params, _), metrics = jax.lax.scan(\n        functools.partial(minibatch_step, normalizer_params=normalizer_params),\n        (optimizer_state, params, key_grad),\n        shuffled_data,\n        length=num_minibatches)\n    return (optimizer_state, params, key), metrics\n\n  def training_step(\n      carry: Tuple[TrainingState, envs.State, PRNGKey],\n      unused_t) -> Tuple[Tuple[TrainingState, envs.State, PRNGKey], Metrics]:\n    training_state, state, key = carry\n    key_sgd, key_generate_unroll, new_key = jax.random.split(key, 3)\n\n    training_policy = make_policy(\n        (training_state.normalizer_params, training_state.params.policy))\n    non_training_policy = make_policy(\n        (non_training_params[0], non_training_params[1]))\n\n    def f(carry, unused_t):\n      current_state, current_key = carry\n      current_key, next_key = jax.random.split(current_key)\n      next_state, data = generate_unroll(\n          env,\n          current_state,\n          training_policy,\n          non_training_policy,\n          current_key,\n          unroll_length,\n          extra_fields=('truncation',))\n      return (next_state, next_key), data\n\n    (state, _), data = jax.lax.scan(\n        f, (state, key_generate_unroll), (),\n        length=batch_size * num_minibatches // num_envs)\n    # Have leading dimensions (batch_size * num_minibatches, unroll_length)\n    data = jax.tree_util.tree_map(lambda x: jnp.swapaxes(x, 1, 2), data)\n    data = jax.tree_util.tree_map(lambda x: jnp.reshape(x, (-1,) + x.shape[2:]),\n                                  data)\n    assert data.discount.shape[1:] == (unroll_length,)\n\n    # Update normalization params and normalize observations.\n    normalizer_params = running_statistics.update(\n        training_state.normalizer_params,\n        data.observation,\n        pmap_axis_name=_PMAP_AXIS_NAME)\n\n    (optimizer_state, params, _), metrics = jax.lax.scan(\n        functools.partial(\n            sgd_step, data=data, normalizer_params=normalizer_params),\n        (training_state.optimizer_state, training_state.params, key_sgd), (),\n        length=num_updates_per_batch)\n\n    new_training_state = TrainingState(\n        optimizer_state=optimizer_state,\n        params=params,\n        normalizer_params=normalizer_params,\n        env_steps=training_state.env_steps + env_step_per_training_step)\n    return (new_training_state, state, new_key), metrics\n\n  def training_epoch(training_state: TrainingState, state: envs.State,\n                     key: PRNGKey) -> Tuple[TrainingState, envs.State, Metrics]:\n    (training_state, state, _), loss_metrics = jax.lax.scan(\n        training_step, (training_state, state, key), (),\n        length=num_training_steps_per_epoch)\n    loss_metrics = jax.tree_util.tree_map(jnp.mean, loss_metrics)\n    return training_state, state, loss_metrics\n\n  training_epoch = jax.pmap(training_epoch, axis_name=_PMAP_AXIS_NAME)\n\n  # Note that this is NOT a pure jittable method.\n  def training_epoch_with_timing(\n      training_state: TrainingState, env_state: envs.State,\n      key: PRNGKey) -> Tuple[TrainingState, envs.State, Metrics]:\n    nonlocal training_walltime\n    t = time.time()\n    training_state, env_state = _strip_weak_type((training_state, env_state))\n    result = training_epoch(training_state, env_state, key)\n    training_state, env_state, metrics = _strip_weak_type(result)\n\n    metrics = jax.tree_util.tree_map(jnp.mean, metrics)\n    jax.tree_util.tree_map(lambda x: x.block_until_ready(), metrics)\n\n    epoch_training_time = time.time() - t\n    training_walltime += epoch_training_time\n    sps = (num_training_steps_per_epoch *\n           env_step_per_training_step *\n           max(num_resets_per_eval, 1)) / epoch_training_time\n    metrics = {\n        'training/sps': sps,\n        'training/walltime': training_walltime,\n        **{f'training/{name}': value for name, value in metrics.items()}\n    }\n    return training_state, env_state, metrics  # pytype: disable=bad-return-type  # py311-upgrade\n    \n\n  training_state = TrainingState(  # pytype: disable=wrong-arg-types  # jax-ndarray\n      optimizer_state=optimizer.init(init_params),  # pytype: disable=wrong-arg-types  # numpy-scalars\n      params=init_params,\n      normalizer_params=normalizer_params,\n      env_steps=0)\n  training_state = jax.device_put_replicated(\n      training_state,\n      jax.local_devices()[:local_devices_to_use])\n\n  if not eval_env:\n    eval_env = environment\n  if randomization_fn is not None:\n    v_randomization_fn = functools.partial(\n        randomization_fn, rng=jax.random.split(eval_key, num_eval_envs)\n    )\n  eval_env = wrap_for_training(\n      eval_env,\n      episode_length=episode_length,\n      action_repeat=action_repeat,\n      randomization_fn=v_randomization_fn,\n  )\n\n  evaluator = Evaluator(\n      eval_env,\n      functools.partial(make_policy, deterministic=deterministic_eval),\n      num_eval_envs=num_eval_envs,\n      episode_length=episode_length,\n      action_repeat=action_repeat,\n      key=eval_key)\n\n  # Run initial eval\n  metrics = {}\n  if process_id == 0 and num_evals > 1:\n    metrics = evaluator.run_evaluation(\n        _unpmap(\n            (training_state.normalizer_params, training_state.params.policy)),\n        (non_training_params[0], non_training_params[1]),\n        training_metrics={})\n    logging.info(metrics)\n    progress_fn(0, metrics)\n\n  training_metrics = {}\n  training_walltime = 0\n  current_step = 0\n  # Initialize variables to allow saving params of run with max score\n  max_score = -99999\n  max_score_params = {}\n  for it in range(num_evals_after_init):\n    logging.info('starting iteration %s %s', it, time.time() - xt)\n\n    for _ in range(max(num_resets_per_eval, 1)):\n      # optimization\n      epoch_key, local_key = jax.random.split(local_key)\n      epoch_keys = jax.random.split(epoch_key, local_devices_to_use)\n      (training_state, env_state, training_metrics) = (\n          training_epoch_with_timing(training_state, env_state, epoch_keys)\n      )\n      current_step = int(_unpmap(training_state.env_steps))\n\n      key_envs = jax.vmap(\n          lambda x, s: jax.random.split(x[0], s),\n          in_axes=(0, None))(key_envs, key_envs.shape[1])\n      # TODO: move extra reset logic to the AutoResetWrapper.\n      env_state = reset_fn(key_envs) if num_resets_per_eval > 0 else env_state\n\n    if process_id == 0:\n      # Run evals.\n      metrics = evaluator.run_evaluation(\n          _unpmap(\n              (training_state.normalizer_params, training_state.params.policy)),\n          (non_training_params[0], non_training_params[1]),\n          training_metrics)\n      logging.info(metrics)\n      progress_fn(current_step, metrics)\n      params = _unpmap(\n          (training_state.normalizer_params, training_state.params.policy,\n           training_state.params.value))\n\n      # Save params if this is the max score\n      eval_score = metrics['eval/episode_reward']\n      if eval_score > max_score:\n        max_score = eval_score\n        max_score_params = {\n            \"score\": max_score,\n            \"params\": params,\n        }\n      policy_params_fn(current_step, make_policy, params)\n\n  total_steps = current_step\n  assert total_steps >= num_timesteps\n\n  # If there was no mistakes the training_state should still be identical on all\n  # devices.\n  pmap.assert_is_replicated(training_state)\n  params = _unpmap(\n      (training_state.normalizer_params, training_state.params.policy,\n       training_state.params.value))\n  logging.info('total steps: %s', total_steps)\n  pmap.synchronize_hosts()\n  return (make_policy, params, metrics, max_score_params)\n","metadata":{"cellView":"form","id":"fxmLdUcPUMSD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the PPO networks (adapted from https://github.com/google/brax)\n\n@flax.struct.dataclass\nclass PPONetworks:\n  policy_network: networks.FeedForwardNetwork\n  value_network: networks.FeedForwardNetwork\n  parametric_action_distribution: distribution.ParametricDistribution\n\ndef make_ppo_networks(\n    observation_size: int,\n    action_size: int,\n    preprocess_observations_fn: types.PreprocessObservationFn = types\n    .identity_observation_preprocessor,\n    policy_hidden_layer_sizes: Sequence[int] = policy_hidden_layer_sizes,\n    value_hidden_layer_sizes: Sequence[int] = value_hidden_layer_sizes,\n    activation: networks.ActivationFn = linen.swish) -> PPONetworks:\n  \"\"\"Make PPO networks with preprocessor.\"\"\"\n  parametric_action_distribution = distribution.NormalTanhDistribution(\n      event_size=action_size)\n  policy_network = networks.make_policy_network(\n      parametric_action_distribution.param_size,\n      observation_size,\n      preprocess_observations_fn=preprocess_observations_fn,\n      hidden_layer_sizes=policy_hidden_layer_sizes,\n      activation=activation)\n  value_network = networks.make_value_network(\n      observation_size,\n      preprocess_observations_fn=preprocess_observations_fn,\n      hidden_layer_sizes=value_hidden_layer_sizes,\n      activation=activation)\n\n  return PPONetworks(\n      policy_network=policy_network,\n      value_network=value_network,\n      parametric_action_distribution=parametric_action_distribution)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7 - Training the Humanoids\n\nMy suggestion is to train the `striker` and `keeper` alternately for a few rounds: \n1. Start with the `striker` and train it until it scores a high percentage of penalties (set `training_agent` to `striker` in section `2 - Config`)\n2. Save and load the neural network params (change `upload_striker_model` to `True` and update `striker_model_path`)\n3. Switch and train the `keeper` until the percentage of scored penalties goes down significantly (change `training_agent` to `keeper` in section `2 - Config`)\n4. Save and load the neural network params (change `upload_keeper_model` to `True` and update `keeper_model_path`)\n5. Go back to step 1\n\nTraining the striker or the keeper for 20m timesteps with 9 evals takes about 35min with two T4 GPUs. Depending on the starting point, that should enable the striker to score and the keeper to stop ~65% of the penalties.","metadata":{}},{"cell_type":"code","source":"# Load params to restart training from a saved checkpoint\n# (i.e. from the saved policy and value neural networks' weights)\n\n# After you train the striker and keeper you'll be able to save and load \n# their neural nets, we recommend to save these as a Dataset \n# (click on the button Upload in the right pane under Input),\n# and update the paths below accordingly\nstriker_model_path = \"/kaggle/input/<dataset_name>/<neural_nets_filename>\"\nkeeper_model_path = \"/kaggle/input/<dataset_name>/<neural_nets_filename>\"\n\ntraining_save_path = \"/kaggle/working/mjx_brax_nn_goals\"\nupload_striker_model = False\nupload_keeper_model = False\n\nif training_agent == \"striker\":\n    if upload_striker_model:\n      striker_params = model.load_params(training_save_path)\n#       striker_params = model.load_params(striker_model_path)\n    else:\n      striker_params = None\n    if upload_keeper_model:\n      keeper_params = model.load_params(keeper_model_path)\n    else:\n      keeper_params = None\nelif training_agent == \"keeper\":    \n    if upload_striker_model:\n      striker_params = model.load_params(striker_model_path)\n    else:\n      striker_params = None\n    if upload_keeper_model:\n#       keeper_params = model.load_params(training_save_path)\n      keeper_params = model.load_params(keeper_model_path)\n    else:\n      keeper_params = None\nelse:\n    raise Exception(f\"{training_agent=} isn't supported.\")","metadata":{"id":"vgeiw_vNjwcq","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train\ntrain_fn = functools.partial(\n    train, num_timesteps=num_timesteps, num_evals=num_evals,\n    episode_length=episode_length, normalize_observations=normalize_observations,\n    action_repeat=action_repeat, unroll_length=unroll_length, num_minibatches=num_minibatches,\n    num_updates_per_batch=num_updates_per_batch, discounting=discounting,\n    learning_rate=learning_rate, entropy_cost=entropy_cost, num_envs=num_envs,\n    reward_scaling=reward_scaling, clipping_epsilon=clipping_epsilon, gae_lambda=gae_lambda,\n    normalize_advantage=normalize_advantage, batch_size=batch_size, seed=0,\n    network_factory=make_ppo_networks, striker_params=striker_params,\n    keeper_params=keeper_params, training_agent=training_agent)\n\n\nx_data = []\ny_data = []\nydataerr = []\ntimes = [datetime.now()]\n\nmax_y, min_y = 5000, 0\ndef progress(num_steps, metrics):\n  times.append(datetime.now())\n  x_data.append(num_steps)\n  y_data.append(metrics['eval/episode_reward'])\n  ydataerr.append(metrics['eval/episode_reward_std'])\n\n  plt.xlim([0, train_fn.keywords['num_timesteps'] * 1.1])\n  plt.ylim([min_y, max_y])\n\n  plt.xlabel('# environment steps')\n  plt.ylabel('reward per episode')\n  plt.title(f'y={y_data[-1]:.1f}')\n\n  plt.errorbar(\n      x_data, y_data, yerr=ydataerr)\n  plt.show()\n\n  if 'training/policy_loss' in metrics:\n    print(\"Other metrics\")\n    print(f\"value loss: {metrics['training/v_loss']:.2f}\")\n    print(f\"max episode reward: {metrics['eval/episode_reward_max']:.0f}\")\n    print(f\"goals: {int(metrics['eval/episode_goal'] * 100)} / 100\")\n\nmake_inference_fn, train_params, _, max_score_params = train_fn(\n    environment=env, progress_fn=progress)\n\nprint(f'time to jit: {times[1] - times[0]}')\nprint(f'time to train: {times[-1] - times[1]}')\nprint(f'total time: {times[-1] - times[0]}\\n')\nprint(f\"max score: {int(max_score_params['score'])}\")","metadata":{"id":"xLiddQYPApBw","cellView":"form","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8 - Save and load the policy\n\nWe can save and load the policy using the brax model API.","metadata":{"id":"YYIch0HEApBx"}},{"cell_type":"code","source":"# Save the model\nmodel.save_params(training_save_path, train_params)\n# model.save_params(training_save_path, max_score_params[\"params\"])","metadata":{"id":"Z8gI6qH6ApBx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the model and define the inference functions\ntraining_inference_fn = make_inference_fn(model.load_params(training_save_path)[:2])\njit_training_inference_fn = jax.jit(training_inference_fn)\n\nif training_agent == \"striker\":    \n    if upload_keeper_model:\n      non_training_inference_fn = make_inference_fn(keeper_params[:2])  \n    else:\n      # Ugly hack for when we don't have non_training_params\n      # (maybe we need some random initialisation for these)\n      non_training_inference_fn = make_inference_fn(model.load_params(training_save_path)[:2])\nelif training_agent == \"keeper\":    \n    if upload_striker_model:\n      non_training_inference_fn = make_inference_fn(striker_params[:2])\n    else:\n      raise Exception(\"You need to upload a striker model to train the keeper\")\nelse:\n    raise Exception(f\"{training_agent=} isn't supported.\")\n\njit_non_training_inference_fn = jax.jit(non_training_inference_fn)","metadata":{"id":"h4reaWgxApBx","cellView":"form","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9 - Visualize the policies\n\nFinally we can visualize the Humanoids in action and watch while they take and stop penalties!\n\nThis can also be saved to an mp4 file which you can then download from the `Output` section (can be found on the right if running in a laptop).","metadata":{"id":"0G357XIfApBy"}},{"cell_type":"code","source":"eval_env = envs.get_environment(env_name)\n\njit_reset = jax.jit(eval_env.reset)\njit_step = jax.jit(eval_env.step)","metadata":{"id":"osYasMw4ApBy","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize the Humanoids and optionally save it to a mp4 file\ninit_time = datetime.now()\nmax_rollout_reward = 0\ngoals = 0\nfor i in range(num_penalties):\n  # Initialize the state\n  rng = jax.random.PRNGKey(i)\n  state = jit_reset(rng)\n  rollout = [state.pipeline_state]\n  total_reward = 0\n\n  # Grab a trajectory\n  n_steps = 100\n  render_every = 2\n\n  for i in range(n_steps):\n    act_rng, rng = jax.random.split(rng)\n    training_ctrl, _ = jit_training_inference_fn(state.obs, act_rng)\n    non_training_ctrl, _ = jit_non_training_inference_fn(state.obs, act_rng)\n    # This is prone to error as the way to concatenate the actions \n    # depends on the order of the actuators in the xml\n    if training_agent == \"striker\":\n      ctrl = jp.concatenate((training_ctrl, non_training_ctrl), axis=0)\n    elif training_agent == \"keeper\":\n      ctrl = jp.concatenate((non_training_ctrl, training_ctrl), axis=0)\n    else:\n      raise Exception(f\"{training_agent=} isn't supported.\") \n\n    assert len(ctrl) == env.action_size\n    state = jit_step(state, ctrl)\n    total_reward += state.metrics[\"reward\"]\n    goals += state.metrics[\"goal\"]\n    rollout.append(state.pipeline_state)\n\n    if state.done:\n      break\n\n  max_rollout_reward = max(max_rollout_reward, total_reward)\n    \n  print(f\"Iteration with reward {int(total_reward)}\")\n  video = env.render(rollout[::render_every], camera=\"back_striker\", height=480, width=640)\n  media.show_video(video, fps=1.0 / env.dt / render_every)\n  media.write_video(f\"/kaggle/working/goals_{int(total_reward)}.mp4\", video, fps=1.0 / env.dt / render_every)\n    \nprint(f\"Max rollout reward was - {int(max_rollout_reward)}\")\nprint(f\"Scored {int(goals)} / {num_penalties} penalties\")\nprint(f'total time: {datetime.now() - init_time}')","metadata":{"id":"d-UhypudApBy","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 10 - Tournament Mode\n\nAs a bonus feature you can also run the environment in Tournament Mode in which two teams of a striker + keeper compete against each other in a penalty shootout. To enable it you need to set `tournament_mode` in section `2 - Config` to `True`. Let's see who wins!\n\nNote: you also need to have saved `striker` and `keeper` neural nets to use in the tournament (see below).","metadata":{}},{"cell_type":"code","source":"def get_jit_inference_func(path: str) -> Callable[jax.Array, str]:\n    inference_fn = make_inference_fn(model.load_params(path)[:2])\n    return jax.jit(inference_fn)\n\n\ndef display_video(camera: str, rollout: list) -> None:\n    vid = env.render(rollout[::render_every], camera=camera, height=480, width=640)\n    media.show_video(vid, fps=1.0 / env.dt / render_every)\n    media.write_video(f\"/kaggle/working/goals_{camera}_{datetime.now()}.mp4\", vid, fps=1.0 / env.dt / render_every)\n\n\ndef penalty(\n    striker_jit_inference_fn: Callable[jax.Array, str],\n    keeper_jit_inference_fn: Callable[jax.Array, str],\n    key: int,\n) -> int:\n    # Initialize the state\n    rng = jax.random.PRNGKey(key)\n    state = jit_reset(rng)\n    rollout = [state.pipeline_state]\n\n    # Grab a trajectory\n    n_steps = 200\n    render_every = 1\n    goal = 0\n    for _ in range(n_steps):\n        act_rng, rng = jax.random.split(rng)\n        striker_ctrl, _ = striker_jit_inference_fn(state.obs, act_rng)\n        keeper_ctrl, _ = keeper_jit_inference_fn(state.obs, rng)\n        ctrl = jp.concatenate((striker_ctrl, keeper_ctrl), axis=0)\n\n        assert len(ctrl) == env.action_size\n        state = jit_step(state, ctrl)\n        goal += state.metrics[\"goal\"]\n        rollout.append(state.pipeline_state)\n\n        if state.done:\n          break\n\n    display_video(\"back_striker\", rollout)\n    display_video(\"side_keeper\", rollout)\n    assert goal <= 1\n    return goal","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if tournament_mode:\n    init_time = datetime.now()\n    \n    # These need to be updated with the neural nets that you trained and saved\n    teamA_striker_path = \"/kaggle/input/<dataset_name>/<neural_nets_filename>\"\n    teamA_keeper_path = \"/kaggle/input/<dataset_name>/<neural_nets_filename>\"\n    teamB_striker_path = \"/kaggle/input/<dataset_name>/<neural_nets_filename>\"\n    teamB_keeper_path = \"/kaggle/input/<dataset_name>/<neural_nets_filename>\"\n    \n    teamA_striker_jit_inference_fn = get_jit_inference_func(teamA_striker_path)\n    teamA_keeper_jit_inference_fn = get_jit_inference_func(teamA_keeper_path)\n    teamB_striker_jit_inference_fn = get_jit_inference_func(teamB_striker_path)\n    teamB_keeper_jit_inference_fn = get_jit_inference_func(teamB_keeper_path)\n\n    goals_teamA = 0\n    goals_teamB = 0\n    for i in range(num_penalties):\n      print(f\"\\nPenalty {i}/{num_penalties}\")\n      goals_teamA += penalty(teamA_striker_jit_inference_fn, teamB_keeper_jit_inference_fn, i)\n      print(f\"Team_A {int(goals_teamA)} - {int(goals_teamB)} Team_B\")\n      \n      goals_teamB += penalty(teamB_striker_jit_inference_fn, teamA_keeper_jit_inference_fn, i*10)\n      print(f\"Team_A {int(goals_teamA)} - {int(goals_teamB)} Team_B\")\n            \n    print(f\"\\nFinal score: Team_A {int(goals_teamA)} - {int(goals_teamB)} Team_B\")\n    print(f\"total time: {datetime.now() - init_time}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}